{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Build your own GPT-4 Tokenizer!"
      ],
      "metadata": {
        "id": "_yeQVl8Cv-AB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install regex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRCpZKauOE3J",
        "outputId": "f7de23ce-6139-4988-b88f-1abdb7e7e53d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "YpgJl8jf1wPd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1\n",
        "\n",
        "Write the BasicTokenizer class, with the following three core functions:\n",
        "\n",
        "def train(self, text, vocab_size, verbose=False)\n",
        "\n",
        "def encode(self, text)\n",
        "\n",
        "def decode(self, ids)\n",
        "\n",
        "Train your tokenizer on whatever text you like and visualize the merged tokens. Do they look reasonable?\n",
        "One default test you may wish to use is the text file tests/taylorswift.txt."
      ],
      "metadata": {
        "id": "Kk_mzb9PJBnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicTokenizer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.vocab = {}  # Mapping from token index to byte string\n",
        "        self.merges = {}  # Mapping from token pairs to new token ID\n",
        "\n",
        "    def get_stats(self, ids):\n",
        "      counts = {}\n",
        "      for pair in zip(ids, ids[1:]):\n",
        "          counts[pair] = counts.get(pair, 0) + 1\n",
        "      return counts\n",
        "\n",
        "    def merge(self, ids, pair, idx):\n",
        "      newids = []\n",
        "      i = 0\n",
        "      while i < len(ids):\n",
        "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "          newids.append(idx)\n",
        "          i += 2\n",
        "        else:\n",
        "          newids.append(ids[i])\n",
        "          i += 1\n",
        "      return newids\n",
        "\n",
        "    def train(self, text, vocab_size, verbose=False):\n",
        "      \"\"\" Train tokenizer using Byte Pair Encoding (BPE). \"\"\"\n",
        "      tokens = list(text.encode(\"utf-8\"))  # Convert text to byte values\n",
        "      self.vocab = {idx: bytes([idx]) for idx in range(256)}  # Initialize with single bytes\n",
        "\n",
        "      vocab_size = 276 # the desired final vocabulary size\n",
        "      num_merges = vocab_size - 256\n",
        "      ids = list(tokens) # copy so we don't destroy the original list\n",
        "\n",
        "      merges = {} # (int, int) -> int\n",
        "      for i in range(num_merges):\n",
        "        stats = self.get_stats(ids)\n",
        "        pair = max(stats, key=stats.get)\n",
        "        idx = 256 + i\n",
        "        #print(f\"merging {pair} into a new token {idx}\")\n",
        "        ids = self.merge(ids, pair, idx)\n",
        "        merges[pair] = idx\n",
        "\n",
        "\n",
        "    def encode(self, text):\n",
        "      \"\"\" Convert text into token IDs. \"\"\"\n",
        "      tokens = list(text.encode(\"utf-8\"))\n",
        "\n",
        "      while len(tokens) >= 2:\n",
        "        stats = self.get_stats(tokens)\n",
        "        pair = min(stats, key=lambda p: self.merges.get(p, float('inf')))\n",
        "\n",
        "        if pair not in self.merges:\n",
        "          break  # Stop merging if the pair is not in the learned merges\n",
        "\n",
        "        tokens = self.merge(tokens, pair, self.merges[pair])\n",
        "\n",
        "      return tokens\n",
        "\n",
        "\n",
        "    def decode(self, ids):\n",
        "      \"\"\" Convert token IDs into text. \"\"\"\n",
        "      tokens = b\"\".join(self.vocab[idx] for idx in ids)\n",
        "\n",
        "      return tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vlVncV6Pv_Cr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    with open(\"/content/drive/MyDrive/dataSet_for_practice/taylorswift.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    tokenizer = BasicTokenizer()\n",
        "    tokenizer.train(text, vocab_size=276, verbose=True)\n",
        "\n",
        "    sample_text = \"Hello, world!\"\n",
        "    encoded = tokenizer.encode(sample_text)\n",
        "    decoded = tokenizer.decode(encoded)\n",
        "\n",
        "    print(\"\\nEncoded:\", encoded)\n",
        "    print(\"Decoded:\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCHGx_o_v_Ap",
        "outputId": "56231e57-1ee7-415e-aa7b-1fc73703dea5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Encoded: [72, 101, 108, 108, 111, 44, 32, 119, 111, 114, 108, 100, 33]\n",
            "Decoded: Hello, world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G6dxRPlO4y1j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2**\n",
        "Convert you BasicTokenizer into a RegexTokenizer, which takes a regex pattern and splits the text exactly as GPT-4 would. Process the parts separately as before, then concatenate the results. Retrain your tokenizer and compare the results before and after. You should see that you will now have no tokens that go across categories (numbers, letters, punctuation, more than one whitespace). Use the GPT-4 pattern:\n",
        "\n",
        "`GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"`\n"
      ],
      "metadata": {
        "id": "DD-07-2KLLCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-4 tokenization pattern\n",
        "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n"
      ],
      "metadata": {
        "id": "mVqSoqByLFxo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RegexTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = {idx: bytes([idx]) for idx in range(256)}  # Initial byte-level vocabulary\n",
        "        self.merges = {}  # Stores merge rules\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Splits text into tokens using GPT-4's regex pattern.\"\"\"\n",
        "        return re.findall(GPT4_SPLIT_PATTERN, text)\n",
        "\n",
        "    def get_stats(self, ids):\n",
        "        \"\"\"Counts occurrences of adjacent token pairs.\"\"\"\n",
        "        counts = {}\n",
        "        for pair in zip(ids, ids[1:]):\n",
        "            counts[pair] = counts.get(pair, 0) + 1\n",
        "        return counts\n",
        "\n",
        "    def merge(self, ids, pair, idx):\n",
        "        \"\"\"Replaces occurrences of a pair with a new token.\"\"\"\n",
        "        newids = []\n",
        "        i = 0\n",
        "        while i < len(ids):\n",
        "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "                newids.append(idx)  # Merge pair\n",
        "                i += 2\n",
        "            else:\n",
        "                newids.append(ids[i])\n",
        "                i += 1\n",
        "        return newids\n",
        "\n",
        "\n",
        "    def train(self, text, vocab_size):\n",
        "        \"\"\"Trains the tokenizer using Byte Pair Encoding (BPE).\"\"\"\n",
        "        tokenized_text = self.tokenize(text)\n",
        "        ids = [byte for word in tokenized_text for byte in word.encode(\"utf-8\")]\n",
        "        vocab_size = 276\n",
        "        num_merges = vocab_size - 256  # Additional tokens needed\n",
        "\n",
        "        for _ in range(num_merges):\n",
        "            stats = self.get_stats(ids)\n",
        "            if not stats:\n",
        "                break  # Stop if no more frequent pairs exist\n",
        "\n",
        "            pair = max(stats, key=stats.get)  # Find most frequent pair\n",
        "            idx = 256 + len(self.merges)  # Assign new token ID\n",
        "            ids = self.merge(ids, pair, idx)  # Merge pairs in text\n",
        "            self.merges[pair] = idx  # Store merge rule\n",
        "\n",
        "            # Update vocabulary with new token\n",
        "            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
        "\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Encodes text into token IDs.\"\"\"\n",
        "        tokenized_text = self.tokenize(text)\n",
        "        tokens = [byte for word in tokenized_text for byte in word.encode(\"utf-8\")]\n",
        "\n",
        "        while len(tokens) >= 2:\n",
        "            stats = self.get_stats(tokens)\n",
        "            pair = min(stats, key=lambda p: self.merges.get(p, float('inf')))\n",
        "            if pair not in self.merges:\n",
        "                break\n",
        "            tokens = self.merge(tokens, pair, self.merges[pair])\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"Decodes token IDs back into text.\"\"\"\n",
        "        tokens = b\"\".join(self.vocab[idx] for idx in ids)\n",
        "        return tokens.decode(\"utf-8\", errors='replace')"
      ],
      "metadata": {
        "id": "yrF9rbk-LFqT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexTokenizer()\n",
        "tokenizer.train(\"Hello world! This is a tokenizer test.\", vocab_size=300)\n",
        "\n",
        "\n",
        "tokenized = tokenizer.tokenize(\"Hello world!\")\n",
        "print(\"Tokenized:\", tokenized)\n",
        "\n",
        "encoded = tokenizer.encode(\"Hello world!\")\n",
        "print(\"Encoded:\", encoded)\n",
        "\n",
        "decoded = tokenizer.decode(encoded)\n",
        "print(\"Decoded:\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvvAdXu6N6uf",
        "outputId": "a2f7c5d7-ff7c-4e26-d02c-5ec71e421833"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized: ['Hello', ' world', '!']\n",
            "Encoded: [269]\n",
            "Decoded: Hello world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3\n",
        "You're now ready to load the merges from the GPT-4 tokenizer and show that your tokenizer produces the identical results for both encode and decode, matching tiktoken.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# match this\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
        "ids = enc.encode(\"hello world!!!? (안녕하세요!) lol123 😉\")\n",
        "text = enc.decode(ids) # get the same text back\n",
        "```\n",
        "\n",
        "Unfortunately, you will run into two issues:\n",
        "\n",
        "It is not trivial to recover the raw merges from the GPT-4 tokenizer. You can easily recover what we call vocab here, and what they call and store under enc._mergeable_ranks. Feel free to copy paste the recover_merges function in minbpe/gpt4.py, which takes these ranks and returns the raw merges. If you wish to know how this function works, read this and this. Basically, under some conditions it is enough to only store the parent nodes (and their rank) and get rid of the precise details of which children merged up to any parent.\n",
        "Second, the GPT-4 tokenizer for some reason permutes its raw bytes. It stores this permutation in the first 256 elements of the mergeable ranks, so you can recover this byte shuffle relatively simply as byte_shuffle = {i: enc._mergeable_ranks[bytes([i])] for i in range(256)}. In both your encode and decode, you'll have to shuffle bytes around accordingly. If you're stuck, reference the"
      ],
      "metadata": {
        "id": "qrc76ZTpoFbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIgWMg02Cql9",
        "outputId": "6068582c-e879-47f6-d840-2122ce031a60"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "KsQ-J_vtoK1X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recover_merges(mergeable_ranks):\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M3wbp03gn41i"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g5HMwmYPoLNS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}