{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mJfxbjNvp0JY"
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from keras.datasets import mnist\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "f2_HCtxkp__L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3mzzzi4qC9g",
        "outputId": "a9f855f3-eaa7-4f35-ddac-bc92732b75c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPMOMAXgqH-G",
        "outputId": "d33a8bd5-2f00-4aa3-cc62-d20204a445ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(train_X[0]),train_y[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "CqTtz3r7qJ30",
        "outputId": "57805d78-9e49-465e-92b2-7a40bb655efc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<matplotlib.image.AxesImage at 0x7b5a935850c0>, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(train_X[1])"
      ],
      "metadata": {
        "id": "UqamRrUHqKb0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(train_X.flatten())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "D-Lmkrk3qKeC",
        "outputId": "28a95b7c-c983-4df5-ac45-f57914b7b3a1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([38847859.,   571880.,   514157.,   438029.,   446644.,   503214.,\n",
              "          469607.,   518723.,   611860.,  4118027.]),\n",
              " array([  0. ,  25.5,  51. ,  76.5, 102. , 127.5, 153. , 178.5, 204. ,\n",
              "        229.5, 255. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGsCAYAAAAPJKchAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAmW0lEQVR4nO3df3DU9Z3H8deCsEFhFxCTTcICURBESIhRYPEUPSMhl2NIe+NwHDOhDtLBCzdQ1DvjtFC1naVl6OlVDuQYTHtKY7EF7vAHlwYDQwmUBDICnozxgKBmE3+xm0RZMPncHx233ZKQbEjyyYbnY+Yz436+n8/3+/5+XHdffvebXYcxxggAAMCSAbYLAAAA1zbCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALAqrsLI/v37NW/ePKWkpMjhcGjnzp0xzf/hD38oh8NxWbvhhht6pmAAANChuAojzc3NysjI0IYNG7o0//HHH1ddXV1Umzx5sh566KFurhQAAHRWXIWR3Nxc/ehHP9K3vvWtNreHw2E9/vjjSk1N1Q033KAZM2aovLw8sn3o0KHyeDyRVl9fr3fffVdLlizppTMAAAB/Ka7CSEeWL1+uiooKlZSU6J133tFDDz2kuXPn6v33329z/JYtW3Trrbfqnnvu6eVKAQDAN/pNGKmtrdVLL72k7du365577tEtt9yixx9/XH/1V3+ll1566bLxFy5c0CuvvMJVEQAALLvOdgHd5fjx42ppadGtt94a1R8Oh3XjjTdeNn7Hjh1qbGzU4sWLe6tEAADQhn4TRpqamjRw4EBVVVVp4MCBUduGDh162fgtW7bob//2b5WUlNRbJQIAgDb0mzCSmZmplpYWNTQ0dHgPyOnTp/X222/rv/7rv3qpOgAA0J64CiNNTU2qqamJPD59+rSqq6s1cuRI3XrrrVq0aJEKCgq0fv16ZWZm6pNPPlFZWZnS09OVl5cXmbd161YlJycrNzfXxmkAAIA/4zDGGNtFdFZ5ebnuv//+y/oXL16s4uJiXbp0ST/60Y/0y1/+Uh999JFGjRqlmTNn6umnn9bUqVMlSa2trRo7dqwKCgr04x//uLdPAQAA/IW4CiMAAKD/6Td/2gsAAOITYQQAAFgVFzewtra26uOPP9awYcPkcDhslwMAADrBGKPGxkalpKRowID2r3/ERRj5+OOP5fV6bZcBAAC64Ny5cxo9enS72+MijAwbNkzSH0/G5XJZrgYAAHRGKBSS1+uNvI+3Jy7CyDcfzbhcLsIIAABxpqNbLK7qBta1a9fK4XBo5cqVVxy3fft2TZo0SQkJCZo6dareeOONqzksAADoR7ocRo4cOaIXX3xR6enpVxx38OBBLVy4UEuWLNGxY8eUn5+v/Px8nThxoquHBgAA/UiXwkhTU5MWLVqk//iP/9CIESOuOPb555/X3Llz9cQTT+i2227Ts88+qzvuuEMvvPBClwoGAAD9S5fCSGFhofLy8pSdnd3h2IqKisvG5eTkqKKiot054XBYoVAoqgEAgP4p5htYS0pKdPToUR05cqRT4wOBgJKSkqL6kpKSFAgE2p3j9/v19NNPx1oaAACIQzFdGTl37pxWrFihV155RQkJCT1Vk4qKihQMBiPt3LlzPXYsAABgV0xXRqqqqtTQ0KA77rgj0tfS0qL9+/frhRdeUDgc1sCBA6PmeDwe1dfXR/XV19fL4/G0exyn0ymn0xlLaQAAIE7FdGXkgQce0PHjx1VdXR1pd955pxYtWqTq6urLgogk+Xw+lZWVRfWVlpbK5/NdXeUAAKBfiOnKyLBhwzRlypSovhtuuEE33nhjpL+goECpqany+/2SpBUrVmj27Nlav3698vLyVFJSosrKSm3evLmbTgEAAMSzbv/V3traWtXV1UUez5o1S9u2bdPmzZuVkZGh1157TTt37rws1AAAgGuTwxhjbBfRkVAoJLfbrWAwyNfBAwAQJzr7/t3tV0YAAABiQRgBAABWEUYAAIBVMX8Da38z7snXbZcQszNr82yXAABAt+HKCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsiimMbNy4Uenp6XK5XHK5XPL5fHrzzTfbHV9cXCyHwxHVEhISrrpoAADQf1wXy+DRo0dr7dq1mjBhgowx+sUvfqH58+fr2LFjuv3229uc43K5dOrUqchjh8NxdRUDAIB+JaYwMm/evKjHP/7xj7Vx40YdOnSo3TDicDjk8Xi6XiEAAOjXunzPSEtLi0pKStTc3Cyfz9fuuKamJo0dO1Zer1fz58/XyZMnO9x3OBxWKBSKagAAoH+KOYwcP35cQ4cOldPp1LJly7Rjxw5Nnjy5zbETJ07U1q1btWvXLr388stqbW3VrFmz9OGHH17xGH6/X263O9K8Xm+sZQIAgDjhMMaYWCZcvHhRtbW1CgaDeu2117Rlyxbt27ev3UDy5y5duqTbbrtNCxcu1LPPPtvuuHA4rHA4HHkcCoXk9XoVDAblcrliKbdD4558vVv31xvOrM2zXQIAAB0KhUJyu90dvn/HdM+IJA0ePFjjx4+XJGVlZenIkSN6/vnn9eKLL3Y4d9CgQcrMzFRNTc0VxzmdTjmdzlhLAwAAceiqv2ektbU16irGlbS0tOj48eNKTk6+2sMCAIB+IqYrI0VFRcrNzdWYMWPU2Niobdu2qby8XHv27JEkFRQUKDU1VX6/X5L0zDPPaObMmRo/frzOnz+vdevW6ezZs3rkkUe6/0wAAEBciimMNDQ0qKCgQHV1dXK73UpPT9eePXv04IMPSpJqa2s1YMCfLrZ88cUXWrp0qQKBgEaMGKGsrCwdPHiwU/eXAACAa0PMN7Da0NkbYLqCG1gBAOgZnX3/5rdpAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYFVMYWTjxo1KT0+Xy+WSy+WSz+fTm2++ecU527dv16RJk5SQkKCpU6fqjTfeuKqCAQBA/xJTGBk9erTWrl2rqqoqVVZW6q//+q81f/58nTx5ss3xBw8e1MKFC7VkyRIdO3ZM+fn5ys/P14kTJ7qleAAAEP8cxhhzNTsYOXKk1q1bpyVLlly2bcGCBWpubtbu3bsjfTNnztS0adO0adOmTh8jFArJ7XYrGAzK5XJdTbmXGffk6926v95wZm2e7RIAAOhQZ9+/u3zPSEtLi0pKStTc3Cyfz9fmmIqKCmVnZ0f15eTkqKKi4or7DofDCoVCUQ0AAPRPMYeR48ePa+jQoXI6nVq2bJl27NihyZMntzk2EAgoKSkpqi8pKUmBQOCKx/D7/XK73ZHm9XpjLRMAAMSJmMPIxIkTVV1drcOHD+vRRx/V4sWL9e6773ZrUUVFRQoGg5F27ty5bt0/AADoO66LdcLgwYM1fvx4SVJWVpaOHDmi559/Xi+++OJlYz0ej+rr66P66uvr5fF4rngMp9Mpp9MZa2kAACAOXfX3jLS2tiocDre5zefzqaysLKqvtLS03XtMAADAtSemKyNFRUXKzc3VmDFj1NjYqG3btqm8vFx79uyRJBUUFCg1NVV+v1+StGLFCs2ePVvr169XXl6eSkpKVFlZqc2bN3f/mQAAgLgUUxhpaGhQQUGB6urq5Ha7lZ6erj179ujBBx+UJNXW1mrAgD9dbJk1a5a2bdum73//+3rqqac0YcIE7dy5U1OmTOneswAAAHHrqr9npDfwPSPR+J4RAEA86PHvGQEAAOgOhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYFVMY8fv9uuuuuzRs2DAlJiYqPz9fp06duuKc4uJiORyOqJaQkHBVRQMAgP4jpjCyb98+FRYW6tChQyotLdWlS5c0Z84cNTc3X3Gey+VSXV1dpJ09e/aqigYAAP3HdbEMfuutt6IeFxcXKzExUVVVVbr33nvbnedwOOTxeLpWIQAA6Neu6p6RYDAoSRo5cuQVxzU1NWns2LHyer2aP3++Tp48ecXx4XBYoVAoqgEAgP6py2GktbVVK1eu1N13360pU6a0O27ixInaunWrdu3apZdfflmtra2aNWuWPvzww3bn+P1+ud3uSPN6vV0tEwAA9HEOY4zpysRHH31Ub775pg4cOKDRo0d3et6lS5d02223aeHChXr22WfbHBMOhxUOhyOPQ6GQvF6vgsGgXC5XV8pt17gnX+/W/fWGM2vzbJcAAECHQqGQ3G53h+/fMd0z8o3ly5dr9+7d2r9/f0xBRJIGDRqkzMxM1dTUtDvG6XTK6XR2pTQAABBnYvqYxhij5cuXa8eOHdq7d6/S0tJiPmBLS4uOHz+u5OTkmOcCAID+J6YrI4WFhdq2bZt27dqlYcOGKRAISJLcbreGDBkiSSooKFBqaqr8fr8k6ZlnntHMmTM1fvx4nT9/XuvWrdPZs2f1yCOPdPOpAACAeBRTGNm4caMk6b777ovqf+mll/Sd73xHklRbW6sBA/50weWLL77Q0qVLFQgENGLECGVlZengwYOaPHny1VUOAAD6hS7fwNqbOnsDTFdwAysAAD2js+/f/DYNAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArIopjPj9ft11110aNmyYEhMTlZ+fr1OnTnU4b/v27Zo0aZISEhI0depUvfHGG10uGAAA9C8xhZF9+/apsLBQhw4dUmlpqS5duqQ5c+aoubm53TkHDx7UwoULtWTJEh07dkz5+fnKz8/XiRMnrrp4AAAQ/xzGGNPVyZ988okSExO1b98+3XvvvW2OWbBggZqbm7V79+5I38yZMzVt2jRt2rSpU8cJhUJyu90KBoNyuVxdLbdN4558vVv31xvOrM2zXQIAAB3q7Pv3Vd0zEgwGJUkjR45sd0xFRYWys7Oj+nJyclRRUdHunHA4rFAoFNUAAED/1OUw0traqpUrV+ruu+/WlClT2h0XCASUlJQU1ZeUlKRAINDuHL/fL7fbHWler7erZQIAgD6uy2GksLBQJ06cUElJSXfWI0kqKipSMBiMtHPnznX7MQAAQN9wXVcmLV++XLt379b+/fs1evToK471eDyqr6+P6quvr5fH42l3jtPplNPp7EppAAAgzsR0ZcQYo+XLl2vHjh3au3ev0tLSOpzj8/lUVlYW1VdaWiqfzxdbpQAAoF+K6cpIYWGhtm3bpl27dmnYsGGR+z7cbreGDBkiSSooKFBqaqr8fr8kacWKFZo9e7bWr1+vvLw8lZSUqLKyUps3b+7mUwEAAPEopisjGzduVDAY1H333afk5ORIe/XVVyNjamtrVVdXF3k8a9Ysbdu2TZs3b1ZGRoZee+017dy584o3vQIAgGtHTFdGOvOVJOXl5Zf1PfTQQ3rooYdiORQAALhG8Ns0AADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsCrmMLJ//37NmzdPKSkpcjgc2rlz5xXHl5eXy+FwXNYCgUBXawYAAP1IzGGkublZGRkZ2rBhQ0zzTp06pbq6ukhLTEyM9dAAAKAfui7WCbm5ucrNzY35QImJiRo+fHjM8wAAQP/Wa/eMTJs2TcnJyXrwwQf1+9///opjw+GwQqFQVAMAAP1Tj4eR5ORkbdq0Sb/5zW/0m9/8Rl6vV/fdd5+OHj3a7hy/3y+32x1pXq+3p8sEAACWOIwxpsuTHQ7t2LFD+fn5Mc2bPXu2xowZo//8z/9sc3s4HFY4HI48DoVC8nq9CgaDcrlcXS23TeOefL1b99cbzqzNs10CAAAdCoVCcrvdHb5/x3zPSHeYPn26Dhw40O52p9Mpp9PZixUBAABbrHzPSHV1tZKTk20cGgAA9DExXxlpampSTU1N5PHp06dVXV2tkSNHasyYMSoqKtJHH32kX/7yl5Kk5557Tmlpabr99tt14cIFbdmyRXv37tX//M//dN9ZAACAuBVzGKmsrNT9998febxq1SpJ0uLFi1VcXKy6ujrV1tZGtl+8eFGPPfaYPvroI11//fVKT0/X7373u6h9AACAa9dV3cDaWzp7A0xXcAMrAAA9o7Pv3/w2DQAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKtiDiP79+/XvHnzlJKSIofDoZ07d3Y4p7y8XHfccYecTqfGjx+v4uLiLpQKAAD6o5jDSHNzszIyMrRhw4ZOjT99+rTy8vJ0//33q7q6WitXrtQjjzyiPXv2xFwsAADof66LdUJubq5yc3M7PX7Tpk1KS0vT+vXrJUm33XabDhw4oH/9139VTk5OrIcHAAD9TI/fM1JRUaHs7OyovpycHFVUVLQ7JxwOKxQKRTUAANA/9XgYCQQCSkpKiupLSkpSKBTSV1991eYcv98vt9sdaV6vt6fLBAAAlvTJv6YpKipSMBiMtHPnztkuCQAA9JCY7xmJlcfjUX19fVRffX29XC6XhgwZ0uYcp9Mpp9PZ06UBAIA+oMevjPh8PpWVlUX1lZaWyufz9fShAQBAHIg5jDQ1Nam6ulrV1dWS/vinu9XV1aqtrZX0x49YCgoKIuOXLVum//u//9M///M/67333tO///u/69e//rW+973vdc8ZAACAuBZzGKmsrFRmZqYyMzMlSatWrVJmZqZWr14tSaqrq4sEE0lKS0vT66+/rtLSUmVkZGj9+vXasmULf9YLAAAkSQ5jjLFdREdCoZDcbreCwaBcLle37nvck6936/56w5m1ebZLAACgQ519/+6Tf00DAACuHYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYRRgBAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYFWXwsiGDRs0btw4JSQkaMaMGfrDH/7Q7tji4mI5HI6olpCQ0OWCAQBA/xJzGHn11Ve1atUqrVmzRkePHlVGRoZycnLU0NDQ7hyXy6W6urpIO3v27FUVDQAA+o+Yw8jPfvYzLV26VA8//LAmT56sTZs26frrr9fWrVvbneNwOOTxeCItKSnpqooGAAD9R0xh5OLFi6qqqlJ2dvafdjBggLKzs1VRUdHuvKamJo0dO1Zer1fz58/XyZMnr3iccDisUCgU1QAAQP8UUxj59NNP1dLSctmVjaSkJAUCgTbnTJw4UVu3btWuXbv08ssvq7W1VbNmzdKHH37Y7nH8fr/cbnekeb3eWMoEAABxpMf/msbn86mgoEDTpk3T7Nmz9dvf/lY33XSTXnzxxXbnFBUVKRgMRtq5c+d6ukwAAGDJdbEMHjVqlAYOHKj6+vqo/vr6enk8nk7tY9CgQcrMzFRNTU27Y5xOp5xOZyylAQCAOBXTlZHBgwcrKytLZWVlkb7W1laVlZXJ5/N1ah8tLS06fvy4kpOTY6sUAAD0SzFdGZGkVatWafHixbrzzjs1ffp0Pffcc2pubtbDDz8sSSooKFBqaqr8fr8k6ZlnntHMmTM1fvx4nT9/XuvWrdPZs2f1yCOPdO+ZAACAuBRzGFmwYIE++eQTrV69WoFAQNOmTdNbb70Vuam1trZWAwb86YLLF198oaVLlyoQCGjEiBHKysrSwYMHNXny5O47CwAAELccxhhju4iOhEIhud1uBYNBuVyubt33uCdf79b99YYza/NslwAAQIc6+/7Nb9MAAACrYv6YBgAAtI8r7rHjyggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAq66zXQBiN+7J122XELMza/NslwAgDsXj6x1i16UwsmHDBq1bt06BQEAZGRn6+c9/runTp7c7fvv27frBD36gM2fOaMKECfrJT36iv/mbv+ly0Yg/vKAAANoT88c0r776qlatWqU1a9bo6NGjysjIUE5OjhoaGtocf/DgQS1cuFBLlizRsWPHlJ+fr/z8fJ04ceKqiwcAAPHPYYwxsUyYMWOG7rrrLr3wwguSpNbWVnm9Xv3TP/2TnnzyycvGL1iwQM3Nzdq9e3ekb+bMmZo2bZo2bdrUqWOGQiG53W4Fg0G5XK5Yyu0Q/8cOALjW9dRH6Z19/47pY5qLFy+qqqpKRUVFkb4BAwYoOztbFRUVbc6pqKjQqlWrovpycnK0c+fOdo8TDocVDocjj4PBoKQ/nlR3aw1/2e37BAAgnvTE++uf77ej6x4xhZFPP/1ULS0tSkpKiupPSkrSe++91+acQCDQ5vhAINDucfx+v55++unL+r1ebyzlAgCATnA/17P7b2xslNvtbnd7n/xrmqKioqirKa2trfr888914403yuFwdNtxQqGQvF6vzp071+0f/+CPWOOexfr2PNa4Z7G+Pcv2+hpj1NjYqJSUlCuOiymMjBo1SgMHDlR9fX1Uf319vTweT5tzPB5PTOMlyel0yul0RvUNHz48llJj4nK5+I+gh7HGPYv17Xmscc9ifXuWzfW90hWRb8T01zSDBw9WVlaWysrKIn2tra0qKyuTz+drc47P54saL0mlpaXtjgcAANeWmD+mWbVqlRYvXqw777xT06dP13PPPafm5mY9/PDDkqSCggKlpqbK7/dLklasWKHZs2dr/fr1ysvLU0lJiSorK7V58+buPRMAABCXYg4jCxYs0CeffKLVq1crEAho2rRpeuuttyI3qdbW1mrAgD9dcJk1a5a2bdum73//+3rqqac0YcIE7dy5U1OmTOm+s+gip9OpNWvWXPaRELoPa9yzWN+exxr3LNa3Z8XL+sb8PSMAAADdiR/KAwAAVhFGAACAVYQRAABgFWEEAABYdU2HkQ0bNmjcuHFKSEjQjBkz9Ic//MF2SXHphz/8oRwOR1SbNGlSZPuFCxdUWFioG2+8UUOHDtXf/d3fXfZFePiT/fv3a968eUpJSZHD4bjsd5yMMVq9erWSk5M1ZMgQZWdn6/33348a8/nnn2vRokVyuVwaPny4lixZoqampl48i76tozX+zne+c9lzeu7cuVFjWOP2+f1+3XXXXRo2bJgSExOVn5+vU6dORY3pzOtCbW2t8vLydP311ysxMVFPPPGEvv766948lT6pM+t73333XfYcXrZsWdSYvrS+12wYefXVV7Vq1SqtWbNGR48eVUZGhnJyctTQ0GC7tLh0++23q66uLtIOHDgQ2fa9731P//3f/63t27dr3759+vjjj/Xtb3/bYrV9W3NzszIyMrRhw4Y2t//0pz/Vv/3bv2nTpk06fPiwbrjhBuXk5OjChQuRMYsWLdLJkydVWlqq3bt3a//+/frud7/bW6fQ53W0xpI0d+7cqOf0r371q6jtrHH79u3bp8LCQh06dEilpaW6dOmS5syZo+bm5siYjl4XWlpalJeXp4sXL+rgwYP6xS9+oeLiYq1evdrGKfUpnVlfSVq6dGnUc/inP/1pZFufW19zjZo+fbopLCyMPG5paTEpKSnG7/dbrCo+rVmzxmRkZLS57fz582bQoEFm+/btkb7//d//NZJMRUVFL1UYvySZHTt2RB63trYaj8dj1q1bF+k7f/68cTqd5le/+pUxxph3333XSDJHjhyJjHnzzTeNw+EwH330Ua/VHi/+co2NMWbx4sVm/vz57c5hjWPT0NBgJJl9+/YZYzr3uvDGG2+YAQMGmEAgEBmzceNG43K5TDgc7t0T6OP+cn2NMWb27NlmxYoV7c7pa+t7TV4ZuXjxoqqqqpSdnR3pGzBggLKzs1VRUWGxsvj1/vvvKyUlRTfffLMWLVqk2tpaSVJVVZUuXboUtdaTJk3SmDFjWOsuOH36tAKBQNR6ut1uzZgxI7KeFRUVGj58uO68887ImOzsbA0YMECHDx/u9ZrjVXl5uRITEzVx4kQ9+uij+uyzzyLbWOPYBINBSdLIkSMlde51oaKiQlOnTo361fecnByFQiGdPHmyF6vv+/5yfb/xyiuvaNSoUZoyZYqKior05ZdfRrb1tfXtk7/a29M+/fRTtbS0RP1LkKSkpCS99957lqqKXzNmzFBxcbEmTpyouro6Pf3007rnnnt04sQJBQIBDR48+LIfOkxKSlIgELBTcBz7Zs3aeu5+sy0QCCgxMTFq+3XXXaeRI0ey5p00d+5cffvb31ZaWpo++OADPfXUU8rNzVVFRYUGDhzIGsegtbVVK1eu1N133x355u3OvC4EAoE2n+ffbMMftbW+kvQP//APGjt2rFJSUvTOO+/oX/7lX3Tq1Cn99re/ldT31veaDCPoXrm5uZF/Tk9P14wZMzR27Fj9+te/1pAhQyxWBnTN3//930f+eerUqUpPT9ctt9yi8vJyPfDAAxYriz+FhYU6ceJE1H1k6D7tre+f3780depUJScn64EHHtAHH3ygW265pbfL7NA1+THNqFGjNHDgwMvu3K6vr5fH47FUVf8xfPhw3XrrraqpqZHH49HFixd1/vz5qDGsddd8s2ZXeu56PJ7LbsT++uuv9fnnn7PmXXTzzTdr1KhRqqmpkcQad9by5cu1e/duvf322xo9enSkvzOvCx6Pp83n+Tfb0P76tmXGjBmSFPUc7kvre02GkcGDBysrK0tlZWWRvtbWVpWVlcnn81msrH9oamrSBx98oOTkZGVlZWnQoEFRa33q1CnV1tay1l2QlpYmj8cTtZ6hUEiHDx+OrKfP59P58+dVVVUVGbN37161trZGXpAQmw8//FCfffaZkpOTJbHGHTHGaPny5dqxY4f27t2rtLS0qO2deV3w+Xw6fvx4VOgrLS2Vy+XS5MmTe+dE+qiO1rct1dXVkhT1HO5T69vrt8z2ESUlJcbpdJri4mLz7rvvmu9+97tm+PDhUXcWo3Mee+wxU15ebk6fPm1+//vfm+zsbDNq1CjT0NBgjDFm2bJlZsyYMWbv3r2msrLS+Hw+4/P5LFfddzU2Nppjx46ZY8eOGUnmZz/7mTl27Jg5e/asMcaYtWvXmuHDh5tdu3aZd955x8yfP9+kpaWZr776KrKPuXPnmszMTHP48GFz4MABM2HCBLNw4UJbp9TnXGmNGxsbzeOPP24qKirM6dOnze9+9ztzxx13mAkTJpgLFy5E9sEat+/RRx81brfblJeXm7q6ukj78ssvI2M6el34+uuvzZQpU8ycOXNMdXW1eeutt8xNN91kioqKbJxSn9LR+tbU1JhnnnnGVFZWmtOnT5tdu3aZm2++2dx7772RffS19b1mw4gxxvz85z83Y8aMMYMHDzbTp083hw4dsl1SXFqwYIFJTk42gwcPNqmpqWbBggWmpqYmsv2rr74y//iP/2hGjBhhrr/+evOtb33L1NXVWay4b3v77beNpMva4sWLjTF//PPeH/zgByYpKck4nU7zwAMPmFOnTkXt47PPPjMLFy40Q4cONS6Xyzz88MOmsbHRwtn0TVda4y+//NLMmTPH3HTTTWbQoEFm7NixZunSpZf9jwpr3L621laSeemllyJjOvO6cObMGZObm2uGDBliRo0aZR577DFz6dKlXj6bvqej9a2trTX33nuvGTlypHE6nWb8+PHmiSeeMMFgMGo/fWl9HcYY03vXYQAAAKJdk/eMAACAvoMwAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACrCCMAAMAqwggAALCKMAIAAKwijAAAAKsIIwAAwKr/B5x4mhgwbpq7AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5enWqfQoqKga",
        "outputId": "2323fa34-be48-4170-ef84-f0e460e60d72"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#0,1 major\n",
        "\n",
        "train_X = torch.tensor(train_X,dtype=torch.float32)/256.\n",
        "train_y = torch.tensor(train_y,dtype=torch.long)\n",
        "\n",
        "test_X =torch.tensor(test_X,dtype=torch.float32)/256.\n",
        "test_y = torch.tensor(test_y,dtype=torch.long)"
      ],
      "metadata": {
        "id": "LZI62S4JqKi6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.hist(train_X.flatten())\n"
      ],
      "metadata": {
        "id": "vPJgzvIOqKlG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-ZPPt0hqKnf",
        "outputId": "e706d229-d06a-4abe-f72e-2df2bee015cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#28*28=784"
      ],
      "metadata": {
        "id": "l5a3BGTHqKpx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X=train_X.reshape([-1,28*28])\n",
        "test_X=test_X.reshape([-1,28*28])"
      ],
      "metadata": {
        "id": "HsmrAjp7qKtL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5R1ld0psLPV",
        "outputId": "182f86a7-81b8-49e5-f656-404792e850da"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZQ17or4sNPD",
        "outputId": "8ecbaccd-8fa0-4526-b776-fd852bb8d48e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([784])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgz610LksNsU",
        "outputId": "e64ce21d-1644-4ddd-ddc9-2b326382170f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0117,\n",
              "        0.0703, 0.0703, 0.0703, 0.4922, 0.5312, 0.6836, 0.1016, 0.6484, 0.9961,\n",
              "        0.9648, 0.4961, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1172, 0.1406, 0.3672, 0.6016,\n",
              "        0.6641, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.8789, 0.6719, 0.9883,\n",
              "        0.9453, 0.7617, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1914, 0.9297, 0.9883, 0.9883,\n",
              "        0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9805, 0.3633, 0.3203,\n",
              "        0.3203, 0.2188, 0.1523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0703, 0.8555, 0.9883,\n",
              "        0.9883, 0.9883, 0.9883, 0.9883, 0.7734, 0.7109, 0.9648, 0.9414, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3125,\n",
              "        0.6094, 0.4180, 0.9883, 0.9883, 0.8008, 0.0430, 0.0000, 0.1680, 0.6016,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0547, 0.0039, 0.6016, 0.9883, 0.3516, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.5430, 0.9883, 0.7422, 0.0078, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0430, 0.7422, 0.9883, 0.2734,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1367, 0.9414,\n",
              "        0.8789, 0.6250, 0.4219, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.3164, 0.9375, 0.9883, 0.9883, 0.4648, 0.0977, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.1758, 0.7266, 0.9883, 0.9883, 0.5859, 0.1055, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0625, 0.3633, 0.9844, 0.9883, 0.7305,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9727, 0.9883,\n",
              "        0.9727, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1797, 0.5078, 0.7148, 0.9883,\n",
              "        0.9883, 0.8086, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.1523, 0.5781, 0.8945, 0.9883, 0.9883,\n",
              "        0.9883, 0.9766, 0.7109, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0938, 0.4453, 0.8633, 0.9883, 0.9883, 0.9883,\n",
              "        0.9883, 0.7852, 0.3047, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0898, 0.2578, 0.8320, 0.9883, 0.9883, 0.9883, 0.9883,\n",
              "        0.7734, 0.3164, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0703, 0.6680, 0.8555, 0.9883, 0.9883, 0.9883, 0.9883, 0.7617,\n",
              "        0.3125, 0.0352, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.2148, 0.6719, 0.8828, 0.9883, 0.9883, 0.9883, 0.9883, 0.9531, 0.5195,\n",
              "        0.0430, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.5312, 0.9883, 0.9883, 0.9883, 0.8281, 0.5273, 0.5156, 0.0625,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# len(set(list(test_y.numpy())))"
      ],
      "metadata": {
        "id": "Qe59OqtjsNuh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the neural network\n",
        "1 hidden layer of neuron size 100\n",
        "output class 10"
      ],
      "metadata": {
        "id": "8S2Ki0bksncF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fc1 = nn.Linear(784,100)\n",
        "final_layer = nn.Linear(100, 10)"
      ],
      "metadata": {
        "id": "WheTR8bvsNwr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size =128\n",
        "\n",
        "random_indexes = np.random.choice(range(0,len(train_y)),128)"
      ],
      "metadata": {
        "id": "dW5PwMTusN0O"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.take(train_X,random_indexes,0)\n",
        "y_train = np.take(train_y,random_indexes,0)\n"
      ],
      "metadata": {
        "id": "oZb3pf-D4D8j"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow8QsoPT4v9s",
        "outputId": "26cfb550-834d-4607-a009-b0c2542c1141"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc1.weight.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxz2OLnl4wA0",
        "outputId": "6a71a508-54b5-4b28-ad93-d8287db488a9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "H1 = fc1(x_train).shape"
      ],
      "metadata": {
        "id": "f4DskUDa4wD3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H1_shape = tuple(H1)\n",
        "\n",
        "H1_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8brLq9cB4wHW",
        "outputId": "536d8682-3b6c-41ad-8122-8f654b4759be"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "95WH3p8q1ctb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_parameters = list(fc1.parameters())+list(final_layer.parameters())\n",
        "all_parameters[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs8wFBEA1DIO",
        "outputId": "466d4028-03ff-40c5-c964-c226b0e4196c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_parameters = list(fc1.parameters())+list(final_layer.parameters())\n",
        "all_parameters[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9hQIX3h1d9h",
        "outputId": "b0ad89b1-0f26-4074-8528-eb85f5a54780"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_parameters[0].flatten()) + len(all_parameters[1].flatten())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4hQ2lb31mzV",
        "outputId": "d819edc6-b3e5-421a-d267-587e090dcd62"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78500"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(all_parameters, lr=.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "FYj5y_PF4XMd"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "h9-2XIF506OT"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H1 = F.relu(fc1(x_train))\n",
        "H2 = F.relu(final_layer(H1))"
      ],
      "metadata": {
        "id": "h-IdUVt-14er"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w93UzUl42p1P",
        "outputId": "c0b2a239-3237-41fe-de64-9ab9e0fd5501"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0000, 0.0102, 0.0564, 0.0000, 0.0678, 0.0049, 0.0000, 0.0000, 0.0000,\n",
              "        0.1008], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output =  F.log_softmax(H2, dim=1)"
      ],
      "metadata": {
        "id": "n4kxvwj12ssD"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " output[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi8qtEQT2win",
        "outputId": "a5ec87c7-284d-4005-c455-1b310aba8e5d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.3272, -2.3170, -2.2708, -2.3272, -2.2594, -2.3223, -2.3272, -2.3272,\n",
              "        -2.3272, -2.2264], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zV06Ixr3Pra",
        "outputId": "89ad574b-b6ee-41b7-bd2b-c13043ca91d9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6, 5, 3, 0, 1, 7, 2, 5, 4, 5, 3, 5, 2, 1, 0, 5, 2, 3, 1, 4, 2, 1, 9, 9,\n",
              "        4, 7, 9, 0, 4, 8, 0, 3, 1, 0, 6, 8, 1, 5, 1, 9, 3, 2, 5, 5, 5, 9, 7, 5,\n",
              "        3, 5, 3, 8, 0, 8, 0, 9, 2, 2, 4, 2, 2, 7, 3, 8, 0, 3, 2, 4, 9, 7, 3, 7,\n",
              "        0, 6, 9, 0, 1, 5, 6, 4, 1, 8, 5, 1, 7, 1, 5, 1, 3, 8, 9, 6, 7, 6, 9, 0,\n",
              "        3, 8, 6, 3, 1, 7, 9, 4, 1, 1, 0, 7, 2, 0, 0, 3, 7, 7, 9, 6, 6, 1, 4, 8,\n",
              "        9, 9, 2, 3, 0, 0, 7, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = criterion(H2, y_train)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNjaX7Gh3Cuw",
        "outputId": "a655b508-a387-405f-8b18-4f38b8b92916"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3085, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fc1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB9wxiVD3NAY",
        "outputId": "8e0f72ce-5038-450b-e395-40f4d2991f60"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=784, out_features=100, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "vM5GLoUl3Xc0"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.step()"
      ],
      "metadata": {
        "id": "9d1FUQSn4SFR"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzE5HqFu4vAj",
        "outputId": "65e818ba-c651-47cc-c031-c7edc8cf0c12"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc1.weight[1][:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw-MrvAZ4yYa",
        "outputId": "cc47a062-f175-4779-f445-2cb4a524d8b3"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0108,  0.0357, -0.0107, -0.0206,  0.0240, -0.0357,  0.0300, -0.0315,\n",
              "        -0.0259, -0.0171,  0.0329, -0.0147, -0.0042,  0.0068, -0.0009,  0.0008,\n",
              "         0.0256, -0.0083, -0.0231,  0.0253, -0.0144,  0.0264,  0.0334, -0.0020,\n",
              "        -0.0288,  0.0333, -0.0135, -0.0112,  0.0289, -0.0324,  0.0121, -0.0011,\n",
              "         0.0318, -0.0268, -0.0274,  0.0290, -0.0194,  0.0339,  0.0007,  0.0355,\n",
              "        -0.0101, -0.0329,  0.0341, -0.0066, -0.0066,  0.0240,  0.0050, -0.0188,\n",
              "         0.0250, -0.0191, -0.0257, -0.0237,  0.0227,  0.0149,  0.0313,  0.0304,\n",
              "         0.0002,  0.0049,  0.0052, -0.0086, -0.0356, -0.0275, -0.0001, -0.0171,\n",
              "         0.0054, -0.0003, -0.0234, -0.0110, -0.0003, -0.0153,  0.0351,  0.0156,\n",
              "        -0.0003,  0.0031,  0.0306, -0.0210, -0.0218,  0.0111, -0.0254, -0.0088,\n",
              "        -0.0245,  0.0142, -0.0177,  0.0223, -0.0159, -0.0304,  0.0152, -0.0170,\n",
              "        -0.0271, -0.0271, -0.0248,  0.0098, -0.0301, -0.0116, -0.0232,  0.0090,\n",
              "         0.0331,  0.0103,  0.0250, -0.0252], grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.step()\n",
        "fc1.weight.grad[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apURXqR542Pw",
        "outputId": "a0f1b8cd-9865-4ad3-a889-021a9ab71310"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -3.6051e-07, -1.6000e-05, -2.4900e-05,  1.9311e-05,\n",
              "         5.3259e-05,  1.9259e-04,  1.6595e-04,  1.6666e-05,  3.2966e-06,\n",
              "         3.3300e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  5.0469e-08,  5.3064e-04,  4.1004e-04,\n",
              "        -5.0416e-05, -1.1277e-04,  5.9451e-05,  1.0359e-04,  1.6745e-04,\n",
              "         1.1780e-04,  1.1797e-04,  1.3126e-04,  1.3745e-04, -1.2226e-05,\n",
              "        -1.1092e-05, -2.3248e-06, -4.3276e-07,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.1984e-06,\n",
              "        -4.6270e-05,  5.1149e-04,  5.1347e-04,  2.7085e-04,  1.5063e-04,\n",
              "         3.2890e-04,  3.6016e-04,  9.4038e-04,  1.0568e-03,  1.0016e-03,\n",
              "         1.3490e-03,  1.5597e-03,  9.1061e-04,  1.2405e-06, -5.2962e-05,\n",
              "        -5.8649e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         3.9798e-07,  7.5169e-07, -6.4505e-05, -8.2433e-05, -1.3258e-04,\n",
              "        -5.1088e-04, -3.4225e-04, -4.1668e-05,  3.7649e-04,  7.1174e-04,\n",
              "         7.7514e-04,  8.2989e-04,  1.0269e-03,  6.4136e-04,  9.1467e-04,\n",
              "         9.0425e-04, -4.4718e-05, -3.3925e-04, -1.8312e-04, -1.3111e-04,\n",
              "        -3.8680e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.7688e-06,  9.7725e-06, -1.0063e-05,\n",
              "        -8.3204e-05, -1.9933e-04, -5.5209e-04, -1.0385e-03, -1.0662e-03,\n",
              "        -4.5013e-04, -5.9041e-04, -4.1702e-04, -3.1335e-04, -2.9438e-04,\n",
              "        -2.2343e-04, -1.3638e-04,  1.7725e-04,  3.8376e-04,  3.4185e-04,\n",
              "        -4.3769e-04, -1.2644e-04, -8.8739e-05, -5.4651e-05,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.2936e-06,\n",
              "         2.8248e-05,  4.6522e-05,  2.6543e-05, -1.9133e-04, -4.9355e-04,\n",
              "        -7.5561e-04, -9.4019e-04, -1.1221e-03, -1.1217e-03, -1.0158e-03,\n",
              "        -1.1588e-03, -1.1052e-03, -9.9901e-04, -1.1249e-03, -4.4370e-04,\n",
              "        -1.1212e-04,  3.2469e-04,  6.0560e-04, -8.7985e-05, -1.6201e-04,\n",
              "        -1.3304e-04, -4.7584e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  3.1881e-05,  6.4530e-05,  5.4600e-05,\n",
              "         6.4489e-05,  1.3691e-05, -2.5712e-04, -5.7998e-04, -7.3820e-04,\n",
              "        -8.5036e-04, -7.8424e-04, -4.5859e-04, -1.5908e-04,  8.9937e-05,\n",
              "        -2.3323e-04, -5.4433e-04,  4.2676e-05, -1.9191e-04,  4.2832e-04,\n",
              "         4.2278e-04, -2.0332e-04, -3.0656e-04, -2.4686e-04, -1.3464e-04,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         2.6279e-05,  2.9695e-05,  5.4561e-05,  4.9864e-04,  4.6252e-04,\n",
              "         5.3017e-04,  2.4213e-04, -3.2755e-04, -7.3968e-04, -4.6580e-04,\n",
              "         2.9282e-04,  5.2512e-04,  5.8965e-04,  5.9249e-04,  1.2946e-04,\n",
              "         9.6264e-06, -1.4074e-04,  4.2047e-04, -1.8265e-04, -5.6824e-04,\n",
              "        -3.4715e-04, -2.1352e-04, -1.1116e-04,  3.0368e-05,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5972e-05, -9.8571e-05,\n",
              "         1.4799e-04,  5.9882e-04,  3.2976e-04,  5.8314e-05, -7.9186e-04,\n",
              "        -1.3083e-03, -1.4079e-03, -8.6933e-04, -1.6897e-04, -1.2660e-04,\n",
              "        -2.5270e-04, -2.7638e-04, -8.2054e-05,  5.3490e-04,  5.8422e-04,\n",
              "         5.2917e-05, -8.5537e-04, -5.5507e-04, -1.4682e-04, -2.7442e-05,\n",
              "         3.5545e-05,  4.5910e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00, -6.3886e-05, -8.1564e-05,  5.0451e-04,  2.1933e-04,\n",
              "        -1.9616e-04, -5.3562e-04, -1.3373e-03, -1.4498e-03, -7.3376e-04,\n",
              "        -4.0476e-04, -1.7523e-04, -7.5014e-05, -1.7791e-04, -7.3505e-04,\n",
              "        -4.0283e-04,  2.2152e-04,  8.5747e-04,  1.6847e-04, -8.0945e-04,\n",
              "        -4.3656e-04, -3.9781e-05, -1.5409e-05,  3.8259e-06,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -9.2553e-05,\n",
              "         3.1375e-04,  5.4051e-04, -2.2424e-04, -3.8442e-04, -7.8663e-04,\n",
              "        -1.3545e-03, -1.8066e-03, -1.3347e-03, -9.0996e-04, -6.0432e-04,\n",
              "        -2.5892e-04, -8.3118e-04, -1.2161e-03, -2.7560e-04,  9.6343e-05,\n",
              "         9.7572e-04, -1.3025e-04, -7.4033e-04, -2.6382e-04, -6.0733e-05,\n",
              "        -2.7717e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -4.1772e-05,  2.1484e-04,  1.4654e-04,\n",
              "        -3.5357e-04, -8.0253e-04, -1.1168e-03, -1.5424e-03, -1.6638e-03,\n",
              "        -1.2715e-03, -9.4316e-04, -5.6297e-04, -4.7665e-04, -6.6934e-04,\n",
              "        -4.6009e-04, -4.2215e-04,  5.3181e-04,  9.3944e-05, -6.8400e-04,\n",
              "        -5.2649e-04, -3.1103e-04, -9.0035e-05, -2.9716e-05,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.8155e-06,  0.0000e+00,\n",
              "         0.0000e+00, -8.9459e-05, -1.4209e-04, -3.4293e-04, -7.3187e-04,\n",
              "        -1.3846e-03, -1.4752e-03, -1.3477e-03, -8.3929e-04, -2.4893e-04,\n",
              "         1.1631e-04,  6.1262e-05, -1.6295e-04, -5.7024e-04,  8.8252e-05,\n",
              "         1.9161e-04, -6.8927e-04, -8.5359e-04, -6.4346e-04, -3.2918e-04,\n",
              "        -9.8180e-05, -1.7745e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1310e-05,\n",
              "        -7.5897e-05, -2.6960e-04, -7.5311e-04, -1.1668e-03, -6.8319e-04,\n",
              "        -3.1844e-04, -5.1307e-05,  5.5429e-05, -1.5937e-04,  5.7673e-04,\n",
              "         4.4484e-04, -1.2242e-04, -2.1490e-04, -6.0784e-04, -6.8305e-04,\n",
              "        -1.0511e-03, -6.6439e-04, -7.6220e-05, -5.9553e-05,  9.3949e-07,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -2.1107e-05, -8.6834e-05,  1.3219e-04,\n",
              "         6.1158e-05,  2.2001e-04,  1.7281e-04,  3.7388e-04,  3.9793e-04,\n",
              "         3.1805e-04,  1.2188e-03,  1.4142e-03,  5.5437e-04, -7.3527e-05,\n",
              "        -7.2882e-04, -5.4437e-04, -4.5327e-04, -3.7600e-04,  5.6640e-05,\n",
              "         4.0174e-04, -8.1820e-06,  2.6234e-07,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         8.3311e-05,  4.4654e-04,  9.4228e-04,  1.3025e-03,  1.1883e-03,\n",
              "         1.1807e-03,  1.2640e-03,  1.5209e-03,  1.8630e-03,  2.2020e-03,\n",
              "         1.3779e-03, -3.8131e-04, -8.7134e-04, -7.8690e-04, -4.1530e-04,\n",
              "        -3.3731e-04, -7.1103e-05,  1.5022e-04,  5.1411e-05,  8.3686e-07,\n",
              "        -7.3546e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  1.8374e-04,  5.7067e-04,  1.2936e-03,\n",
              "         1.3915e-03,  1.2159e-03,  7.5303e-04,  1.8250e-03,  2.2463e-03,\n",
              "         2.5312e-03,  2.3980e-03,  1.9186e-03,  6.0979e-04, -6.2211e-04,\n",
              "        -7.3433e-04, -5.3656e-04, -4.6380e-04, -5.1787e-04, -2.7831e-04,\n",
              "        -2.2889e-04, -6.9158e-05,  2.0084e-05,  1.2369e-05,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         6.1404e-04,  9.3592e-04,  1.4359e-03,  1.3418e-03,  9.1723e-04,\n",
              "         9.1098e-04,  2.1836e-03,  2.8129e-03,  2.8797e-03,  1.9410e-03,\n",
              "         9.4078e-04, -1.9562e-04, -1.0120e-03, -7.2998e-04, -7.1455e-04,\n",
              "        -8.1724e-04, -2.0362e-05,  4.1255e-04,  7.7423e-04,  7.1918e-04,\n",
              "         5.1958e-04,  4.3032e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  5.4273e-04,  1.0444e-03,\n",
              "         1.2196e-03,  9.8049e-04,  8.0918e-04,  5.5753e-04,  1.1568e-03,\n",
              "         2.3971e-03,  2.4049e-03,  1.0837e-03, -2.0444e-04, -4.0895e-04,\n",
              "        -4.9620e-04, -8.7298e-05,  2.2440e-04,  6.4326e-04,  1.3792e-03,\n",
              "         1.6504e-03,  1.7092e-03,  1.7974e-03,  1.3635e-03,  3.1925e-04,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  8.5474e-05,  1.0461e-03,  8.0087e-04,  4.4259e-04,\n",
              "         2.1208e-04,  2.9881e-04,  5.5445e-05,  2.2580e-05, -5.7176e-05,\n",
              "        -3.8503e-04, -7.7214e-04, -1.4328e-03, -1.4616e-03, -1.1710e-03,\n",
              "        -4.8857e-04,  5.3661e-04,  1.0866e-03,  1.1662e-03,  1.4513e-03,\n",
              "         1.5186e-03,  8.8911e-04,  5.5830e-04,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1184e-04,\n",
              "         4.5273e-04,  2.8617e-04, -3.4097e-05, -3.8775e-04, -7.7612e-04,\n",
              "        -1.1934e-03, -1.4070e-03, -1.6088e-03, -1.5728e-03, -1.8470e-03,\n",
              "        -2.1563e-03, -1.7154e-03, -1.2190e-03, -6.3107e-04, -1.6739e-04,\n",
              "         2.4989e-04,  3.9767e-04,  7.2909e-04,  8.0276e-04,  2.6557e-04,\n",
              "         0.0000e+00, -1.2169e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -5.0096e-05,  8.6906e-06,  4.0467e-04,\n",
              "         1.9211e-04, -4.8345e-04, -1.2628e-03, -1.5077e-03, -1.5688e-03,\n",
              "        -1.6928e-03, -1.8039e-03, -1.9883e-03, -1.9946e-03, -1.3781e-03,\n",
              "        -8.8283e-04, -4.9560e-04, -2.8755e-04, -2.5767e-04, -1.6211e-04,\n",
              "        -6.4190e-05, -1.0210e-05,  5.0853e-06,  0.0000e+00, -2.8481e-06,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -8.1761e-06, -1.4371e-04, -4.6293e-04,\n",
              "        -6.8435e-04, -7.6098e-04, -9.9486e-04, -1.0978e-03, -1.1308e-03,\n",
              "        -9.0848e-04, -8.7384e-04, -5.5282e-04, -2.6407e-04, -2.6553e-04,\n",
              "        -2.1902e-04, -1.7659e-04, -1.1766e-04, -7.0780e-05, -3.8576e-05,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00, -7.0664e-05, -1.8277e-04, -1.3226e-04,\n",
              "        -2.3667e-04, -3.5685e-04, -3.4652e-04, -3.1844e-04, -3.1199e-04,\n",
              "        -2.2412e-04, -8.4905e-05, -7.2917e-05, -8.1012e-05, -6.1911e-05,\n",
              "        -3.2751e-05, -2.1834e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  7.0697e-06, -4.8618e-06, -2.0480e-04, -1.8672e-04,\n",
              "        -4.4483e-05, -9.9180e-05, -7.5084e-05, -8.7499e-05, -2.5595e-05,\n",
              "        -2.2995e-05, -5.0041e-05, -3.8465e-05, -1.8695e-05,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc1 = nn.Linear(784, 100)\n",
        "final_layer = nn.Linear(100, 10)\n",
        "optimizer = optim.SGD(list(fc1.parameters())+list(final_layer.parameters()), lr=.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "pISm_yKB4_WV"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "running_accuracy = []\n",
        "\n",
        "for iteration in range(10000 ):\n",
        "\n",
        "  random_indexes = np.random.choice(range(0,len(train_y)),128)\n",
        "\n",
        "  x_train = Variable(np.take(train_X,random_indexes,0))\n",
        "  y_train = Variable(np.take(train_y,random_indexes,0))\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  H1 = F.relu(fc1(x_train))\n",
        "  H2 = F.relu(final_layer(H1))\n",
        "\n",
        "  predicted_class = torch.argmax(H2,1)\n",
        "  accuracy_running_one = sum(predicted_class == y_train)/128*100\n",
        "  running_accuracy.append(accuracy_running_one)\n",
        "\n",
        "\n",
        "  # output =  F.log_softmax(H2, dim=1)\n",
        "  loss = criterion(H2, y_train)\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  if iteration%100 == 0:\n",
        "\n",
        "    accuracy = sum(running_accuracy)/len(running_accuracy)\n",
        "    running_accuracy = []\n",
        "\n",
        "\n",
        "    H1 = F.relu(fc1(test_X))\n",
        "    H2 = F.relu(final_layer(H1))\n",
        "\n",
        "    predicted_class = torch.argmax(H2,1)\n",
        "    test_accuracy = sum(predicted_class == test_y)/test_y.shape[0]*100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print (\"The iteration number is {} | and the loss it {}| Trainning Accuracy {}| Test Accuracy {}\".format(iteration, loss.item(),accuracy,test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CL55FbS5DjG",
        "outputId": "b8d770d8-fd96-4877-931d-23cb5e3b1acc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The iteration number is 0 | and the loss it 2.3013200759887695| Trainning Accuracy 7.03125| Test Accuracy 7.7699995040893555\n",
            "The iteration number is 100 | and the loss it 2.297807455062866| Trainning Accuracy 9.078125| Test Accuracy 11.519999504089355\n",
            "The iteration number is 200 | and the loss it 2.2832419872283936| Trainning Accuracy 13.390625| Test Accuracy 16.1200008392334\n",
            "The iteration number is 300 | and the loss it 2.254507303237915| Trainning Accuracy 17.84375| Test Accuracy 20.530000686645508\n",
            "The iteration number is 400 | and the loss it 2.2461116313934326| Trainning Accuracy 22.84375| Test Accuracy 27.73999786376953\n",
            "The iteration number is 500 | and the loss it 2.23577618598938| Trainning Accuracy 30.78125| Test Accuracy 35.53000259399414\n",
            "The iteration number is 600 | and the loss it 2.2179980278015137| Trainning Accuracy 38.21875| Test Accuracy 43.90999984741211\n",
            "The iteration number is 700 | and the loss it 2.212531328201294| Trainning Accuracy 45.6640625| Test Accuracy 49.349998474121094\n",
            "The iteration number is 800 | and the loss it 2.188138246536255| Trainning Accuracy 50.6015625| Test Accuracy 52.71999740600586\n",
            "The iteration number is 900 | and the loss it 2.177539825439453| Trainning Accuracy 53.6484375| Test Accuracy 55.44000244140625\n",
            "The iteration number is 1000 | and the loss it 2.1230475902557373| Trainning Accuracy 55.484375| Test Accuracy 57.11000061035156\n",
            "The iteration number is 1100 | and the loss it 2.098031759262085| Trainning Accuracy 57.53125| Test Accuracy 58.679996490478516\n",
            "The iteration number is 1200 | and the loss it 2.098410129547119| Trainning Accuracy 58.484375| Test Accuracy 60.07999801635742\n",
            "The iteration number is 1300 | and the loss it 2.0931358337402344| Trainning Accuracy 59.8671875| Test Accuracy 60.4900016784668\n",
            "The iteration number is 1400 | and the loss it 2.105515480041504| Trainning Accuracy 60.375| Test Accuracy 60.939998626708984\n",
            "The iteration number is 1500 | and the loss it 2.0666091442108154| Trainning Accuracy 60.6953125| Test Accuracy 61.25\n",
            "The iteration number is 1600 | and the loss it 2.027156114578247| Trainning Accuracy 61.125| Test Accuracy 61.599998474121094\n",
            "The iteration number is 1700 | and the loss it 1.9704240560531616| Trainning Accuracy 61.34375| Test Accuracy 61.68000030517578\n",
            "The iteration number is 1800 | and the loss it 2.0142955780029297| Trainning Accuracy 60.6953125| Test Accuracy 61.959999084472656\n",
            "The iteration number is 1900 | and the loss it 1.9421683549880981| Trainning Accuracy 61.6875| Test Accuracy 62.30999755859375\n",
            "The iteration number is 2000 | and the loss it 1.971968412399292| Trainning Accuracy 61.859375| Test Accuracy 62.44000244140625\n",
            "The iteration number is 2100 | and the loss it 1.8600940704345703| Trainning Accuracy 61.859375| Test Accuracy 62.540000915527344\n",
            "The iteration number is 2200 | and the loss it 1.938156008720398| Trainning Accuracy 62.6171875| Test Accuracy 62.62000274658203\n",
            "The iteration number is 2300 | and the loss it 1.8469383716583252| Trainning Accuracy 62.7734375| Test Accuracy 62.80000305175781\n",
            "The iteration number is 2400 | and the loss it 1.8343647718429565| Trainning Accuracy 62.78125| Test Accuracy 62.91999816894531\n",
            "The iteration number is 2500 | and the loss it 1.8893256187438965| Trainning Accuracy 62.96875| Test Accuracy 63.08000183105469\n",
            "The iteration number is 2600 | and the loss it 1.8568737506866455| Trainning Accuracy 62.5546875| Test Accuracy 63.279998779296875\n",
            "The iteration number is 2700 | and the loss it 1.7322611808776855| Trainning Accuracy 62.921875| Test Accuracy 63.529998779296875\n",
            "The iteration number is 2800 | and the loss it 1.8560259342193604| Trainning Accuracy 63.28125| Test Accuracy 63.650001525878906\n",
            "The iteration number is 2900 | and the loss it 1.747775673866272| Trainning Accuracy 63.7109375| Test Accuracy 63.66999816894531\n",
            "The iteration number is 3000 | and the loss it 1.6209535598754883| Trainning Accuracy 63.7421875| Test Accuracy 63.88999938964844\n",
            "The iteration number is 3100 | and the loss it 1.6744279861450195| Trainning Accuracy 63.359375| Test Accuracy 64.24000549316406\n",
            "The iteration number is 3200 | and the loss it 1.710343837738037| Trainning Accuracy 63.828125| Test Accuracy 64.3800048828125\n",
            "The iteration number is 3300 | and the loss it 1.6127758026123047| Trainning Accuracy 64.25| Test Accuracy 64.66000366210938\n",
            "The iteration number is 3400 | and the loss it 1.6769847869873047| Trainning Accuracy 64.0546875| Test Accuracy 64.70999908447266\n",
            "The iteration number is 3500 | and the loss it 1.5151878595352173| Trainning Accuracy 64.0859375| Test Accuracy 64.74000549316406\n",
            "The iteration number is 3600 | and the loss it 1.5888813734054565| Trainning Accuracy 64.828125| Test Accuracy 64.83000183105469\n",
            "The iteration number is 3700 | and the loss it 1.5448262691497803| Trainning Accuracy 63.6484375| Test Accuracy 65.16000366210938\n",
            "The iteration number is 3800 | and the loss it 1.6831645965576172| Trainning Accuracy 64.3671875| Test Accuracy 65.29000091552734\n",
            "The iteration number is 3900 | and the loss it 1.5330835580825806| Trainning Accuracy 64.953125| Test Accuracy 65.52999877929688\n",
            "The iteration number is 4000 | and the loss it 1.4725838899612427| Trainning Accuracy 64.3671875| Test Accuracy 65.75\n",
            "The iteration number is 4100 | and the loss it 1.4466768503189087| Trainning Accuracy 65.2890625| Test Accuracy 66.04999542236328\n",
            "The iteration number is 4200 | and the loss it 1.3688123226165771| Trainning Accuracy 66.4765625| Test Accuracy 66.29999542236328\n",
            "The iteration number is 4300 | and the loss it 1.393693208694458| Trainning Accuracy 65.9765625| Test Accuracy 66.61000061035156\n",
            "The iteration number is 4400 | and the loss it 1.4156293869018555| Trainning Accuracy 66.890625| Test Accuracy 66.86000061035156\n",
            "The iteration number is 4500 | and the loss it 1.5898211002349854| Trainning Accuracy 66.75| Test Accuracy 67.15999603271484\n",
            "The iteration number is 4600 | and the loss it 1.4487481117248535| Trainning Accuracy 66.078125| Test Accuracy 67.12999725341797\n",
            "The iteration number is 4700 | and the loss it 1.2926216125488281| Trainning Accuracy 67.140625| Test Accuracy 67.30999755859375\n",
            "The iteration number is 4800 | and the loss it 1.358710527420044| Trainning Accuracy 67.84375| Test Accuracy 67.54999542236328\n",
            "The iteration number is 4900 | and the loss it 1.2959303855895996| Trainning Accuracy 67.546875| Test Accuracy 67.7699966430664\n",
            "The iteration number is 5000 | and the loss it 1.3645026683807373| Trainning Accuracy 66.7265625| Test Accuracy 68.0199966430664\n",
            "The iteration number is 5100 | and the loss it 1.443910002708435| Trainning Accuracy 67.671875| Test Accuracy 68.08999633789062\n",
            "The iteration number is 5200 | and the loss it 1.3683608770370483| Trainning Accuracy 67.5390625| Test Accuracy 68.19000244140625\n",
            "The iteration number is 5300 | and the loss it 1.3111374378204346| Trainning Accuracy 67.9921875| Test Accuracy 68.29000091552734\n",
            "The iteration number is 5400 | and the loss it 1.218685269355774| Trainning Accuracy 68.234375| Test Accuracy 68.48999786376953\n",
            "The iteration number is 5500 | and the loss it 1.3480734825134277| Trainning Accuracy 68.9375| Test Accuracy 68.6500015258789\n",
            "The iteration number is 5600 | and the loss it 1.2837278842926025| Trainning Accuracy 68.7109375| Test Accuracy 68.73999786376953\n",
            "The iteration number is 5700 | and the loss it 1.2287392616271973| Trainning Accuracy 68.7890625| Test Accuracy 68.91000366210938\n",
            "The iteration number is 5800 | and the loss it 1.1348037719726562| Trainning Accuracy 69.0625| Test Accuracy 68.9800033569336\n",
            "The iteration number is 5900 | and the loss it 1.259995937347412| Trainning Accuracy 69.5078125| Test Accuracy 69.05000305175781\n",
            "The iteration number is 6000 | and the loss it 1.2343422174453735| Trainning Accuracy 68.8359375| Test Accuracy 69.25\n",
            "The iteration number is 6100 | and the loss it 1.2392631769180298| Trainning Accuracy 69.4140625| Test Accuracy 69.30999755859375\n",
            "The iteration number is 6200 | and the loss it 1.2936033010482788| Trainning Accuracy 68.4453125| Test Accuracy 69.51000213623047\n",
            "The iteration number is 6300 | and the loss it 1.2313487529754639| Trainning Accuracy 68.7890625| Test Accuracy 69.54000091552734\n",
            "The iteration number is 6400 | and the loss it 1.2847721576690674| Trainning Accuracy 68.953125| Test Accuracy 69.6500015258789\n",
            "The iteration number is 6500 | and the loss it 1.2132787704467773| Trainning Accuracy 69.5234375| Test Accuracy 69.77999877929688\n",
            "The iteration number is 6600 | and the loss it 1.1623347997665405| Trainning Accuracy 69.34375| Test Accuracy 69.84000396728516\n",
            "The iteration number is 6700 | and the loss it 1.1747596263885498| Trainning Accuracy 69.171875| Test Accuracy 69.91999816894531\n",
            "The iteration number is 6800 | and the loss it 1.2659209966659546| Trainning Accuracy 70.28125| Test Accuracy 70.02999877929688\n",
            "The iteration number is 6900 | and the loss it 0.9781674146652222| Trainning Accuracy 69.8984375| Test Accuracy 70.12000274658203\n",
            "The iteration number is 7000 | and the loss it 1.1343443393707275| Trainning Accuracy 69.9765625| Test Accuracy 70.19000244140625\n",
            "The iteration number is 7100 | and the loss it 1.065787672996521| Trainning Accuracy 70.484375| Test Accuracy 70.2300033569336\n",
            "The iteration number is 7200 | and the loss it 1.1000670194625854| Trainning Accuracy 69.3828125| Test Accuracy 70.3499984741211\n",
            "The iteration number is 7300 | and the loss it 1.3567912578582764| Trainning Accuracy 70.6171875| Test Accuracy 70.44000244140625\n",
            "The iteration number is 7400 | and the loss it 1.0389041900634766| Trainning Accuracy 70.5546875| Test Accuracy 70.59000396728516\n",
            "The iteration number is 7500 | and the loss it 1.1197103261947632| Trainning Accuracy 69.28125| Test Accuracy 70.68000030517578\n",
            "The iteration number is 7600 | and the loss it 1.2026958465576172| Trainning Accuracy 70.6953125| Test Accuracy 70.80000305175781\n",
            "The iteration number is 7700 | and the loss it 1.0593366622924805| Trainning Accuracy 70.6484375| Test Accuracy 70.77000427246094\n",
            "The iteration number is 7800 | and the loss it 1.2762869596481323| Trainning Accuracy 70.3828125| Test Accuracy 70.83000183105469\n",
            "The iteration number is 7900 | and the loss it 1.2108656167984009| Trainning Accuracy 70.3984375| Test Accuracy 70.92000579833984\n",
            "The iteration number is 8000 | and the loss it 0.9584723114967346| Trainning Accuracy 71.140625| Test Accuracy 70.93000030517578\n",
            "The iteration number is 8100 | and the loss it 0.9723513126373291| Trainning Accuracy 70.8828125| Test Accuracy 71.0\n",
            "The iteration number is 8200 | and the loss it 0.9870430827140808| Trainning Accuracy 71.2109375| Test Accuracy 71.10000610351562\n",
            "The iteration number is 8300 | and the loss it 1.0257115364074707| Trainning Accuracy 71.3984375| Test Accuracy 71.08000183105469\n",
            "The iteration number is 8400 | and the loss it 1.030988335609436| Trainning Accuracy 70.453125| Test Accuracy 71.12000274658203\n",
            "The iteration number is 8500 | and the loss it 1.1224855184555054| Trainning Accuracy 70.8671875| Test Accuracy 71.20000457763672\n",
            "The iteration number is 8600 | and the loss it 1.0358245372772217| Trainning Accuracy 70.875| Test Accuracy 71.21000671386719\n",
            "The iteration number is 8700 | and the loss it 1.110504150390625| Trainning Accuracy 70.0703125| Test Accuracy 71.20000457763672\n",
            "The iteration number is 8800 | and the loss it 0.9457190036773682| Trainning Accuracy 71.140625| Test Accuracy 71.31999969482422\n",
            "The iteration number is 8900 | and the loss it 0.9488525986671448| Trainning Accuracy 70.640625| Test Accuracy 71.4000015258789\n",
            "The iteration number is 9000 | and the loss it 1.134609341621399| Trainning Accuracy 71.515625| Test Accuracy 71.43000030517578\n",
            "The iteration number is 9100 | and the loss it 1.0090932846069336| Trainning Accuracy 70.765625| Test Accuracy 71.41999816894531\n",
            "The iteration number is 9200 | and the loss it 1.0684071779251099| Trainning Accuracy 71.8671875| Test Accuracy 71.4800033569336\n",
            "The iteration number is 9300 | and the loss it 1.049655795097351| Trainning Accuracy 71.90625| Test Accuracy 71.61000061035156\n",
            "The iteration number is 9400 | and the loss it 1.0428805351257324| Trainning Accuracy 70.7578125| Test Accuracy 71.6300048828125\n",
            "The iteration number is 9500 | and the loss it 1.1526525020599365| Trainning Accuracy 71.078125| Test Accuracy 71.69999694824219\n",
            "The iteration number is 9600 | and the loss it 1.1163614988327026| Trainning Accuracy 71.1171875| Test Accuracy 71.68000030517578\n",
            "The iteration number is 9700 | and the loss it 0.8937098383903503| Trainning Accuracy 71.671875| Test Accuracy 71.80000305175781\n",
            "The iteration number is 9800 | and the loss it 1.1648255586624146| Trainning Accuracy 71.65625| Test Accuracy 71.80000305175781\n",
            "The iteration number is 9900 | and the loss it 1.0595916509628296| Trainning Accuracy 71.796875| Test Accuracy 71.83000183105469\n",
            "The iteration number is 10000 | and the loss it 1.0298751592636108| Trainning Accuracy 72.1640625| Test Accuracy 71.83999633789062\n",
            "The iteration number is 10100 | and the loss it 0.9913887977600098| Trainning Accuracy 71.875| Test Accuracy 71.87999725341797\n",
            "The iteration number is 10200 | and the loss it 0.9678616523742676| Trainning Accuracy 71.28125| Test Accuracy 71.88999938964844\n",
            "The iteration number is 10300 | and the loss it 0.9300358295440674| Trainning Accuracy 72.0| Test Accuracy 71.87999725341797\n",
            "The iteration number is 10400 | and the loss it 1.1678756475448608| Trainning Accuracy 71.71875| Test Accuracy 71.8499984741211\n",
            "The iteration number is 10500 | and the loss it 0.9456527233123779| Trainning Accuracy 71.953125| Test Accuracy 71.91000366210938\n",
            "The iteration number is 10600 | and the loss it 1.0324287414550781| Trainning Accuracy 71.65625| Test Accuracy 71.94000244140625\n",
            "The iteration number is 10700 | and the loss it 0.9529462456703186| Trainning Accuracy 71.8125| Test Accuracy 71.88999938964844\n",
            "The iteration number is 10800 | and the loss it 1.0532411336898804| Trainning Accuracy 71.0390625| Test Accuracy 72.0\n",
            "The iteration number is 10900 | and the loss it 0.9966502785682678| Trainning Accuracy 71.34375| Test Accuracy 72.0199966430664\n",
            "The iteration number is 11000 | and the loss it 0.9914395809173584| Trainning Accuracy 72.296875| Test Accuracy 72.0999984741211\n",
            "The iteration number is 11100 | and the loss it 1.0418189764022827| Trainning Accuracy 71.9140625| Test Accuracy 72.1500015258789\n",
            "The iteration number is 11200 | and the loss it 0.8789451122283936| Trainning Accuracy 71.3125| Test Accuracy 72.15999603271484\n",
            "The iteration number is 11300 | and the loss it 0.8419632315635681| Trainning Accuracy 72.03125| Test Accuracy 72.22000122070312\n",
            "The iteration number is 11400 | and the loss it 0.9665706157684326| Trainning Accuracy 71.515625| Test Accuracy 72.29999542236328\n",
            "The iteration number is 11500 | and the loss it 0.831545889377594| Trainning Accuracy 71.6484375| Test Accuracy 72.29999542236328\n",
            "The iteration number is 11600 | and the loss it 1.0074963569641113| Trainning Accuracy 72.3359375| Test Accuracy 72.3499984741211\n",
            "The iteration number is 11700 | and the loss it 0.9168651700019836| Trainning Accuracy 72.75| Test Accuracy 72.3699951171875\n",
            "The iteration number is 11800 | and the loss it 1.082183599472046| Trainning Accuracy 72.4140625| Test Accuracy 72.43000030517578\n",
            "The iteration number is 11900 | and the loss it 1.0194286108016968| Trainning Accuracy 72.3671875| Test Accuracy 72.40999603271484\n",
            "The iteration number is 12000 | and the loss it 1.2509956359863281| Trainning Accuracy 72.0078125| Test Accuracy 72.45999908447266\n",
            "The iteration number is 12100 | and the loss it 0.7835426330566406| Trainning Accuracy 71.90625| Test Accuracy 72.48999786376953\n",
            "The iteration number is 12200 | and the loss it 0.9981498122215271| Trainning Accuracy 72.453125| Test Accuracy 72.50999450683594\n",
            "The iteration number is 12300 | and the loss it 0.9240293502807617| Trainning Accuracy 72.859375| Test Accuracy 72.54999542236328\n",
            "The iteration number is 12400 | and the loss it 0.9038443565368652| Trainning Accuracy 71.765625| Test Accuracy 72.54999542236328\n",
            "The iteration number is 12500 | and the loss it 0.9117083549499512| Trainning Accuracy 72.40625| Test Accuracy 72.5999984741211\n",
            "The iteration number is 12600 | and the loss it 1.0622631311416626| Trainning Accuracy 72.09375| Test Accuracy 72.66999816894531\n",
            "The iteration number is 12700 | and the loss it 0.9687022566795349| Trainning Accuracy 72.0| Test Accuracy 72.68999481201172\n",
            "The iteration number is 12800 | and the loss it 1.0336389541625977| Trainning Accuracy 72.5546875| Test Accuracy 72.77999877929688\n",
            "The iteration number is 12900 | and the loss it 0.7867926359176636| Trainning Accuracy 72.0390625| Test Accuracy 72.79000091552734\n",
            "The iteration number is 13000 | and the loss it 0.7847857475280762| Trainning Accuracy 72.828125| Test Accuracy 72.81999969482422\n",
            "The iteration number is 13100 | and the loss it 1.0088438987731934| Trainning Accuracy 73.1796875| Test Accuracy 72.8499984741211\n",
            "The iteration number is 13200 | and the loss it 0.9311262369155884| Trainning Accuracy 72.234375| Test Accuracy 72.82999420166016\n",
            "The iteration number is 13300 | and the loss it 0.8375075459480286| Trainning Accuracy 73.4140625| Test Accuracy 72.83999633789062\n",
            "The iteration number is 13400 | and the loss it 0.7844655513763428| Trainning Accuracy 72.7734375| Test Accuracy 72.86000061035156\n",
            "The iteration number is 13500 | and the loss it 0.9661695957183838| Trainning Accuracy 72.796875| Test Accuracy 72.87999725341797\n",
            "The iteration number is 13600 | and the loss it 0.7409734129905701| Trainning Accuracy 72.7578125| Test Accuracy 72.88999938964844\n",
            "The iteration number is 13700 | and the loss it 0.9229936003684998| Trainning Accuracy 72.5859375| Test Accuracy 72.93999481201172\n",
            "The iteration number is 13800 | and the loss it 0.817548930644989| Trainning Accuracy 72.90625| Test Accuracy 72.93999481201172\n",
            "The iteration number is 13900 | and the loss it 0.9293328523635864| Trainning Accuracy 72.734375| Test Accuracy 72.93000030517578\n",
            "The iteration number is 14000 | and the loss it 0.8866408467292786| Trainning Accuracy 72.0| Test Accuracy 72.93999481201172\n",
            "The iteration number is 14100 | and the loss it 0.8733810186386108| Trainning Accuracy 72.3671875| Test Accuracy 72.94999694824219\n",
            "The iteration number is 14200 | and the loss it 0.9906941652297974| Trainning Accuracy 72.8984375| Test Accuracy 72.98999786376953\n",
            "The iteration number is 14300 | and the loss it 0.9514238238334656| Trainning Accuracy 72.90625| Test Accuracy 73.04000091552734\n",
            "The iteration number is 14400 | and the loss it 0.9871512651443481| Trainning Accuracy 72.0625| Test Accuracy 73.04999542236328\n",
            "The iteration number is 14500 | and the loss it 1.0292757749557495| Trainning Accuracy 73.1328125| Test Accuracy 73.07999420166016\n",
            "The iteration number is 14600 | and the loss it 1.046647548675537| Trainning Accuracy 73.2890625| Test Accuracy 73.07999420166016\n",
            "The iteration number is 14700 | and the loss it 0.8544484376907349| Trainning Accuracy 72.6953125| Test Accuracy 73.07999420166016\n",
            "The iteration number is 14800 | and the loss it 1.090441346168518| Trainning Accuracy 72.9765625| Test Accuracy 73.11000061035156\n",
            "The iteration number is 14900 | and the loss it 0.8590919375419617| Trainning Accuracy 72.2265625| Test Accuracy 73.07999420166016\n",
            "The iteration number is 15000 | and the loss it 0.8783581852912903| Trainning Accuracy 72.421875| Test Accuracy 73.12999725341797\n",
            "The iteration number is 15100 | and the loss it 0.7757575511932373| Trainning Accuracy 72.5078125| Test Accuracy 73.1199951171875\n",
            "The iteration number is 15200 | and the loss it 0.917725145816803| Trainning Accuracy 72.5703125| Test Accuracy 73.22000122070312\n",
            "The iteration number is 15300 | and the loss it 0.93958580493927| Trainning Accuracy 72.375| Test Accuracy 73.23999786376953\n",
            "The iteration number is 15400 | and the loss it 0.7090929746627808| Trainning Accuracy 73.125| Test Accuracy 73.27999877929688\n",
            "The iteration number is 15500 | and the loss it 1.0300770998001099| Trainning Accuracy 73.4375| Test Accuracy 73.29000091552734\n",
            "The iteration number is 15600 | and the loss it 0.9345002174377441| Trainning Accuracy 72.671875| Test Accuracy 73.31999969482422\n",
            "The iteration number is 15700 | and the loss it 1.0042539834976196| Trainning Accuracy 73.6328125| Test Accuracy 73.27999877929688\n",
            "The iteration number is 15800 | and the loss it 0.7362862229347229| Trainning Accuracy 73.484375| Test Accuracy 73.27999877929688\n",
            "The iteration number is 15900 | and the loss it 0.8776953816413879| Trainning Accuracy 72.921875| Test Accuracy 73.31999969482422\n",
            "The iteration number is 16000 | and the loss it 0.8427724242210388| Trainning Accuracy 72.984375| Test Accuracy 73.29999542236328\n",
            "The iteration number is 16100 | and the loss it 0.8493794798851013| Trainning Accuracy 73.1171875| Test Accuracy 73.31999969482422\n",
            "The iteration number is 16200 | and the loss it 0.7644014954566956| Trainning Accuracy 72.9921875| Test Accuracy 73.31999969482422\n",
            "The iteration number is 16300 | and the loss it 0.8682509660720825| Trainning Accuracy 73.125| Test Accuracy 73.3699951171875\n",
            "The iteration number is 16400 | and the loss it 0.8383376598358154| Trainning Accuracy 73.0703125| Test Accuracy 73.36000061035156\n",
            "The iteration number is 16500 | and the loss it 0.8666157722473145| Trainning Accuracy 72.9375| Test Accuracy 73.37999725341797\n",
            "The iteration number is 16600 | and the loss it 0.8324781656265259| Trainning Accuracy 73.0234375| Test Accuracy 73.37999725341797\n",
            "The iteration number is 16700 | and the loss it 0.831235945224762| Trainning Accuracy 73.84375| Test Accuracy 73.38999938964844\n",
            "The iteration number is 16800 | and the loss it 0.8545172810554504| Trainning Accuracy 73.125| Test Accuracy 73.4000015258789\n",
            "The iteration number is 16900 | and the loss it 0.6665196418762207| Trainning Accuracy 73.3984375| Test Accuracy 73.4000015258789\n",
            "The iteration number is 17000 | and the loss it 0.8092195987701416| Trainning Accuracy 72.8671875| Test Accuracy 73.43000030517578\n",
            "The iteration number is 17100 | and the loss it 0.7593490481376648| Trainning Accuracy 72.6640625| Test Accuracy 73.45999908447266\n",
            "The iteration number is 17200 | and the loss it 0.8101299405097961| Trainning Accuracy 72.640625| Test Accuracy 73.47000122070312\n",
            "The iteration number is 17300 | and the loss it 0.8292664885520935| Trainning Accuracy 72.59375| Test Accuracy 73.47999572753906\n",
            "The iteration number is 17400 | and the loss it 0.8344789743423462| Trainning Accuracy 72.78125| Test Accuracy 73.55999755859375\n",
            "The iteration number is 17500 | and the loss it 0.5868016481399536| Trainning Accuracy 73.34375| Test Accuracy 73.54000091552734\n",
            "The iteration number is 17600 | and the loss it 0.8440444469451904| Trainning Accuracy 73.5234375| Test Accuracy 73.56999969482422\n",
            "The iteration number is 17700 | and the loss it 0.9780172109603882| Trainning Accuracy 72.875| Test Accuracy 73.55999755859375\n",
            "The iteration number is 17800 | and the loss it 0.8561923503875732| Trainning Accuracy 73.0390625| Test Accuracy 73.5999984741211\n",
            "The iteration number is 17900 | and the loss it 0.8602398633956909| Trainning Accuracy 73.0078125| Test Accuracy 73.61000061035156\n",
            "The iteration number is 18000 | and the loss it 0.921371579170227| Trainning Accuracy 73.7109375| Test Accuracy 73.6500015258789\n",
            "The iteration number is 18100 | and the loss it 0.8600736260414124| Trainning Accuracy 73.6796875| Test Accuracy 73.6500015258789\n",
            "The iteration number is 18200 | and the loss it 1.1059250831604004| Trainning Accuracy 72.734375| Test Accuracy 73.66999816894531\n",
            "The iteration number is 18300 | and the loss it 0.8940703272819519| Trainning Accuracy 73.2578125| Test Accuracy 73.65999603271484\n",
            "The iteration number is 18400 | and the loss it 0.8754028677940369| Trainning Accuracy 73.578125| Test Accuracy 73.69999694824219\n",
            "The iteration number is 18500 | and the loss it 0.957402229309082| Trainning Accuracy 72.7109375| Test Accuracy 73.70999908447266\n",
            "The iteration number is 18600 | and the loss it 0.703088641166687| Trainning Accuracy 73.625| Test Accuracy 73.68999481201172\n",
            "The iteration number is 18700 | and the loss it 1.0646588802337646| Trainning Accuracy 73.109375| Test Accuracy 73.69999694824219\n",
            "The iteration number is 18800 | and the loss it 0.7767923474311829| Trainning Accuracy 72.921875| Test Accuracy 73.66999816894531\n",
            "The iteration number is 18900 | and the loss it 0.8291658759117126| Trainning Accuracy 73.546875| Test Accuracy 73.65999603271484\n",
            "The iteration number is 19000 | and the loss it 0.8921990990638733| Trainning Accuracy 73.609375| Test Accuracy 73.66999816894531\n",
            "The iteration number is 19100 | and the loss it 0.8562430143356323| Trainning Accuracy 72.8515625| Test Accuracy 73.68999481201172\n",
            "The iteration number is 19200 | and the loss it 0.7520075440406799| Trainning Accuracy 73.609375| Test Accuracy 73.72999572753906\n",
            "The iteration number is 19300 | and the loss it 0.6465859413146973| Trainning Accuracy 73.6328125| Test Accuracy 73.80999755859375\n",
            "The iteration number is 19400 | and the loss it 0.6707223057746887| Trainning Accuracy 73.46875| Test Accuracy 73.79000091552734\n",
            "The iteration number is 19500 | and the loss it 0.851864755153656| Trainning Accuracy 72.53125| Test Accuracy 73.79999542236328\n",
            "The iteration number is 19600 | and the loss it 0.6892544031143188| Trainning Accuracy 73.546875| Test Accuracy 73.83000183105469\n",
            "The iteration number is 19700 | and the loss it 0.9815855622291565| Trainning Accuracy 73.78125| Test Accuracy 73.83000183105469\n",
            "The iteration number is 19800 | and the loss it 0.7926031351089478| Trainning Accuracy 73.40625| Test Accuracy 73.83999633789062\n",
            "The iteration number is 19900 | and the loss it 0.8595609068870544| Trainning Accuracy 72.71875| Test Accuracy 73.88999938964844\n",
            "The iteration number is 20000 | and the loss it 0.9748428463935852| Trainning Accuracy 73.0| Test Accuracy 73.80999755859375\n",
            "The iteration number is 20100 | and the loss it 0.9369669556617737| Trainning Accuracy 72.7265625| Test Accuracy 73.76000213623047\n",
            "The iteration number is 20200 | and the loss it 0.8438183665275574| Trainning Accuracy 73.109375| Test Accuracy 73.80999755859375\n",
            "The iteration number is 20300 | and the loss it 0.7338128089904785| Trainning Accuracy 73.328125| Test Accuracy 73.90999603271484\n",
            "The iteration number is 20400 | and the loss it 0.7712591290473938| Trainning Accuracy 73.515625| Test Accuracy 73.93000030517578\n",
            "The iteration number is 20500 | and the loss it 0.850136399269104| Trainning Accuracy 73.46875| Test Accuracy 73.88999938964844\n",
            "The iteration number is 20600 | and the loss it 0.9189034104347229| Trainning Accuracy 73.3125| Test Accuracy 73.88999938964844\n",
            "The iteration number is 20700 | and the loss it 0.7798847556114197| Trainning Accuracy 73.234375| Test Accuracy 73.93000030517578\n",
            "The iteration number is 20800 | and the loss it 0.8020788431167603| Trainning Accuracy 73.4921875| Test Accuracy 73.91999816894531\n",
            "The iteration number is 20900 | and the loss it 0.8881711363792419| Trainning Accuracy 73.8828125| Test Accuracy 73.93000030517578\n",
            "The iteration number is 21000 | and the loss it 0.894264280796051| Trainning Accuracy 72.5078125| Test Accuracy 73.94999694824219\n",
            "The iteration number is 21100 | and the loss it 0.7598767280578613| Trainning Accuracy 73.2578125| Test Accuracy 73.93000030517578\n",
            "The iteration number is 21200 | and the loss it 0.8619781732559204| Trainning Accuracy 74.1015625| Test Accuracy 73.94999694824219\n",
            "The iteration number is 21300 | and the loss it 0.7751396298408508| Trainning Accuracy 72.7421875| Test Accuracy 73.97999572753906\n",
            "The iteration number is 21400 | and the loss it 0.7960953116416931| Trainning Accuracy 73.3125| Test Accuracy 73.95999908447266\n",
            "The iteration number is 21500 | and the loss it 0.9628574252128601| Trainning Accuracy 73.5625| Test Accuracy 73.94999694824219\n",
            "The iteration number is 21600 | and the loss it 0.708490252494812| Trainning Accuracy 73.59375| Test Accuracy 73.9000015258789\n",
            "The iteration number is 21700 | and the loss it 0.6406492590904236| Trainning Accuracy 73.28125| Test Accuracy 73.93000030517578\n",
            "The iteration number is 21800 | and the loss it 1.0629589557647705| Trainning Accuracy 74.2578125| Test Accuracy 73.90999603271484\n",
            "The iteration number is 21900 | and the loss it 0.8182594776153564| Trainning Accuracy 73.2734375| Test Accuracy 73.94000244140625\n",
            "The iteration number is 22000 | and the loss it 1.0447396039962769| Trainning Accuracy 73.53125| Test Accuracy 73.97000122070312\n",
            "The iteration number is 22100 | and the loss it 0.9334193468093872| Trainning Accuracy 73.6953125| Test Accuracy 73.98999786376953\n",
            "The iteration number is 22200 | and the loss it 0.8995206356048584| Trainning Accuracy 73.1640625| Test Accuracy 74.0\n",
            "The iteration number is 22300 | and the loss it 0.8371013402938843| Trainning Accuracy 74.3046875| Test Accuracy 74.0\n",
            "The iteration number is 22400 | and the loss it 0.9055922031402588| Trainning Accuracy 73.71875| Test Accuracy 74.05999755859375\n",
            "The iteration number is 22500 | and the loss it 0.5684109926223755| Trainning Accuracy 73.484375| Test Accuracy 74.0\n",
            "The iteration number is 22600 | and the loss it 0.9642250537872314| Trainning Accuracy 72.890625| Test Accuracy 74.02999877929688\n",
            "The iteration number is 22700 | and the loss it 0.5636795163154602| Trainning Accuracy 73.515625| Test Accuracy 74.0\n",
            "The iteration number is 22800 | and the loss it 0.7852969169616699| Trainning Accuracy 73.1484375| Test Accuracy 74.01000213623047\n",
            "The iteration number is 22900 | and the loss it 0.8236743807792664| Trainning Accuracy 73.9765625| Test Accuracy 74.04000091552734\n",
            "The iteration number is 23000 | and the loss it 0.6309453248977661| Trainning Accuracy 73.25| Test Accuracy 74.06999969482422\n",
            "The iteration number is 23100 | and the loss it 0.76414555311203| Trainning Accuracy 73.84375| Test Accuracy 74.1500015258789\n",
            "The iteration number is 23200 | and the loss it 0.6655740141868591| Trainning Accuracy 74.390625| Test Accuracy 74.05999755859375\n",
            "The iteration number is 23300 | and the loss it 0.7532197833061218| Trainning Accuracy 74.3203125| Test Accuracy 74.08999633789062\n",
            "The iteration number is 23400 | and the loss it 0.9187523126602173| Trainning Accuracy 74.2265625| Test Accuracy 74.1199951171875\n",
            "The iteration number is 23500 | and the loss it 0.8388830423355103| Trainning Accuracy 74.40625| Test Accuracy 74.13999938964844\n",
            "The iteration number is 23600 | and the loss it 0.8262845873832703| Trainning Accuracy 73.515625| Test Accuracy 74.0999984741211\n",
            "The iteration number is 23700 | and the loss it 0.6271566152572632| Trainning Accuracy 73.3515625| Test Accuracy 74.1500015258789\n",
            "The iteration number is 23800 | and the loss it 0.8831202387809753| Trainning Accuracy 74.1015625| Test Accuracy 74.11000061035156\n",
            "The iteration number is 23900 | and the loss it 0.9032548069953918| Trainning Accuracy 74.4375| Test Accuracy 74.1500015258789\n",
            "The iteration number is 24000 | and the loss it 0.7925200462341309| Trainning Accuracy 74.0703125| Test Accuracy 74.16999816894531\n",
            "The iteration number is 24100 | and the loss it 0.8804993033409119| Trainning Accuracy 73.3828125| Test Accuracy 74.13999938964844\n",
            "The iteration number is 24200 | and the loss it 0.8282738327980042| Trainning Accuracy 73.2578125| Test Accuracy 74.13999938964844\n",
            "The iteration number is 24300 | and the loss it 0.8545657396316528| Trainning Accuracy 73.609375| Test Accuracy 74.16999816894531\n",
            "The iteration number is 24400 | and the loss it 0.9097337126731873| Trainning Accuracy 73.34375| Test Accuracy 74.15999603271484\n",
            "The iteration number is 24500 | and the loss it 0.6820706725120544| Trainning Accuracy 74.0390625| Test Accuracy 74.18000030517578\n",
            "The iteration number is 24600 | and the loss it 0.8704003691673279| Trainning Accuracy 73.9609375| Test Accuracy 74.18000030517578\n",
            "The iteration number is 24700 | and the loss it 0.7843767404556274| Trainning Accuracy 73.875| Test Accuracy 74.1500015258789\n",
            "The iteration number is 24800 | and the loss it 0.7181878685951233| Trainning Accuracy 73.6953125| Test Accuracy 74.22000122070312\n",
            "The iteration number is 24900 | and the loss it 0.6871480941772461| Trainning Accuracy 73.3359375| Test Accuracy 74.22000122070312\n",
            "The iteration number is 25000 | and the loss it 0.8642137050628662| Trainning Accuracy 73.5234375| Test Accuracy 74.22999572753906\n",
            "The iteration number is 25100 | and the loss it 0.6645747423171997| Trainning Accuracy 73.921875| Test Accuracy 74.20999908447266\n",
            "The iteration number is 25200 | and the loss it 0.7654397487640381| Trainning Accuracy 73.71875| Test Accuracy 74.22999572753906\n",
            "The iteration number is 25300 | and the loss it 0.7779698967933655| Trainning Accuracy 74.0234375| Test Accuracy 74.22000122070312\n",
            "The iteration number is 25400 | and the loss it 0.9153051972389221| Trainning Accuracy 73.625| Test Accuracy 74.2699966430664\n",
            "The iteration number is 25500 | and the loss it 0.7949034571647644| Trainning Accuracy 74.2734375| Test Accuracy 74.30999755859375\n",
            "The iteration number is 25600 | and the loss it 0.8817526698112488| Trainning Accuracy 74.2109375| Test Accuracy 74.2699966430664\n",
            "The iteration number is 25700 | and the loss it 0.8632103800773621| Trainning Accuracy 73.40625| Test Accuracy 74.2699966430664\n",
            "The iteration number is 25800 | and the loss it 0.8732708692550659| Trainning Accuracy 73.53125| Test Accuracy 74.30999755859375\n",
            "The iteration number is 25900 | and the loss it 0.6525273323059082| Trainning Accuracy 73.953125| Test Accuracy 74.31999969482422\n",
            "The iteration number is 26000 | and the loss it 0.8801124691963196| Trainning Accuracy 73.90625| Test Accuracy 74.31999969482422\n",
            "The iteration number is 26100 | and the loss it 0.7567086815834045| Trainning Accuracy 74.171875| Test Accuracy 74.29999542236328\n",
            "The iteration number is 26200 | and the loss it 0.8849518299102783| Trainning Accuracy 74.0390625| Test Accuracy 74.33000183105469\n",
            "The iteration number is 26300 | and the loss it 0.8728794455528259| Trainning Accuracy 74.3828125| Test Accuracy 74.33000183105469\n",
            "The iteration number is 26400 | and the loss it 0.7713570594787598| Trainning Accuracy 73.8671875| Test Accuracy 74.37999725341797\n",
            "The iteration number is 26500 | and the loss it 0.7586400508880615| Trainning Accuracy 74.2421875| Test Accuracy 74.40999603271484\n",
            "The iteration number is 26600 | and the loss it 0.8134105801582336| Trainning Accuracy 73.53125| Test Accuracy 74.3499984741211\n",
            "The iteration number is 26700 | and the loss it 0.5548786520957947| Trainning Accuracy 75.09375| Test Accuracy 74.37000274658203\n",
            "The iteration number is 26800 | and the loss it 0.7183849215507507| Trainning Accuracy 74.5703125| Test Accuracy 74.3499984741211\n",
            "The iteration number is 26900 | and the loss it 0.6939188241958618| Trainning Accuracy 74.109375| Test Accuracy 74.37999725341797\n",
            "The iteration number is 27000 | and the loss it 0.8930383324623108| Trainning Accuracy 74.0703125| Test Accuracy 74.37000274658203\n",
            "The iteration number is 27100 | and the loss it 0.8609276413917542| Trainning Accuracy 73.8515625| Test Accuracy 74.37000274658203\n",
            "The iteration number is 27200 | and the loss it 0.6162155270576477| Trainning Accuracy 73.9296875| Test Accuracy 74.37000274658203\n",
            "The iteration number is 27300 | and the loss it 0.8132277727127075| Trainning Accuracy 74.0625| Test Accuracy 74.38999938964844\n",
            "The iteration number is 27400 | and the loss it 0.7822008728981018| Trainning Accuracy 74.8046875| Test Accuracy 74.4000015258789\n",
            "The iteration number is 27500 | and the loss it 1.0271003246307373| Trainning Accuracy 74.15625| Test Accuracy 74.37999725341797\n",
            "The iteration number is 27600 | and the loss it 0.7963908910751343| Trainning Accuracy 73.953125| Test Accuracy 74.40999603271484\n",
            "The iteration number is 27700 | and the loss it 0.9714366793632507| Trainning Accuracy 73.796875| Test Accuracy 74.41999816894531\n",
            "The iteration number is 27800 | and the loss it 0.8732110857963562| Trainning Accuracy 73.96875| Test Accuracy 74.40999603271484\n",
            "The iteration number is 27900 | and the loss it 0.6574651002883911| Trainning Accuracy 73.75| Test Accuracy 74.4000015258789\n",
            "The iteration number is 28000 | and the loss it 0.8409282565116882| Trainning Accuracy 73.859375| Test Accuracy 74.38999938964844\n",
            "The iteration number is 28100 | and the loss it 0.7812886834144592| Trainning Accuracy 73.96875| Test Accuracy 74.41999816894531\n",
            "The iteration number is 28200 | and the loss it 0.7667754888534546| Trainning Accuracy 74.453125| Test Accuracy 74.38999938964844\n",
            "The iteration number is 28300 | and the loss it 0.8973881602287292| Trainning Accuracy 74.0546875| Test Accuracy 74.41999816894531\n",
            "The iteration number is 28400 | and the loss it 0.7090874314308167| Trainning Accuracy 74.3203125| Test Accuracy 74.40999603271484\n",
            "The iteration number is 28500 | and the loss it 1.0205985307693481| Trainning Accuracy 74.234375| Test Accuracy 74.43000030517578\n",
            "The iteration number is 28600 | and the loss it 0.7344159483909607| Trainning Accuracy 73.7265625| Test Accuracy 74.44999694824219\n",
            "The iteration number is 28700 | and the loss it 0.6819868683815002| Trainning Accuracy 73.515625| Test Accuracy 74.45999908447266\n",
            "The iteration number is 28800 | and the loss it 0.6236028671264648| Trainning Accuracy 73.859375| Test Accuracy 74.44999694824219\n",
            "The iteration number is 28900 | and the loss it 0.9633236527442932| Trainning Accuracy 73.7890625| Test Accuracy 74.43000030517578\n",
            "The iteration number is 29000 | and the loss it 0.7046099901199341| Trainning Accuracy 73.7890625| Test Accuracy 74.40999603271484\n",
            "The iteration number is 29100 | and the loss it 0.8111748695373535| Trainning Accuracy 74.5234375| Test Accuracy 74.44000244140625\n",
            "The iteration number is 29200 | and the loss it 0.8430066108703613| Trainning Accuracy 73.7109375| Test Accuracy 74.44999694824219\n",
            "The iteration number is 29300 | and the loss it 0.7289277911186218| Trainning Accuracy 73.96875| Test Accuracy 74.48999786376953\n",
            "The iteration number is 29400 | and the loss it 0.8755396604537964| Trainning Accuracy 74.3984375| Test Accuracy 74.47999572753906\n",
            "The iteration number is 29500 | and the loss it 0.9862771034240723| Trainning Accuracy 73.453125| Test Accuracy 74.44999694824219\n",
            "The iteration number is 29600 | and the loss it 0.687735378742218| Trainning Accuracy 74.3515625| Test Accuracy 74.47000122070312\n",
            "The iteration number is 29700 | and the loss it 0.7690661549568176| Trainning Accuracy 74.3046875| Test Accuracy 74.47000122070312\n",
            "The iteration number is 29800 | and the loss it 0.8302072286605835| Trainning Accuracy 74.4765625| Test Accuracy 74.44000244140625\n",
            "The iteration number is 29900 | and the loss it 0.7271876335144043| Trainning Accuracy 73.4765625| Test Accuracy 74.45999908447266\n",
            "The iteration number is 30000 | and the loss it 0.7415995597839355| Trainning Accuracy 74.7421875| Test Accuracy 74.44999694824219\n",
            "The iteration number is 30100 | and the loss it 0.8301413059234619| Trainning Accuracy 73.90625| Test Accuracy 74.5\n",
            "The iteration number is 30200 | and the loss it 0.7058366537094116| Trainning Accuracy 73.96875| Test Accuracy 74.48999786376953\n",
            "The iteration number is 30300 | and the loss it 0.8463859558105469| Trainning Accuracy 74.25| Test Accuracy 74.51000213623047\n",
            "The iteration number is 30400 | and the loss it 0.9027520418167114| Trainning Accuracy 73.171875| Test Accuracy 74.5\n",
            "The iteration number is 30500 | and the loss it 0.7186761498451233| Trainning Accuracy 73.8984375| Test Accuracy 74.51000213623047\n",
            "The iteration number is 30600 | and the loss it 0.8573330044746399| Trainning Accuracy 73.9375| Test Accuracy 74.51000213623047\n",
            "The iteration number is 30700 | and the loss it 0.8600647449493408| Trainning Accuracy 73.765625| Test Accuracy 74.47999572753906\n",
            "The iteration number is 30800 | and the loss it 0.6835489869117737| Trainning Accuracy 74.46875| Test Accuracy 74.5199966430664\n",
            "The iteration number is 30900 | and the loss it 0.562995433807373| Trainning Accuracy 73.7109375| Test Accuracy 74.5199966430664\n",
            "The iteration number is 31000 | and the loss it 0.7503350973129272| Trainning Accuracy 74.3984375| Test Accuracy 74.56999969482422\n",
            "The iteration number is 31100 | and the loss it 0.8333219289779663| Trainning Accuracy 74.5078125| Test Accuracy 74.62999725341797\n",
            "The iteration number is 31200 | and the loss it 0.792149543762207| Trainning Accuracy 74.0859375| Test Accuracy 74.58000183105469\n",
            "The iteration number is 31300 | and the loss it 0.8365793824195862| Trainning Accuracy 74.3984375| Test Accuracy 74.58999633789062\n",
            "The iteration number is 31400 | and the loss it 0.7942243218421936| Trainning Accuracy 74.03125| Test Accuracy 74.5999984741211\n",
            "The iteration number is 31500 | and the loss it 0.9080633521080017| Trainning Accuracy 74.0390625| Test Accuracy 74.58999633789062\n",
            "The iteration number is 31600 | and the loss it 0.7619907855987549| Trainning Accuracy 74.15625| Test Accuracy 74.5999984741211\n",
            "The iteration number is 31700 | and the loss it 0.532198429107666| Trainning Accuracy 73.9375| Test Accuracy 74.65999603271484\n",
            "The iteration number is 31800 | and the loss it 0.7836875915527344| Trainning Accuracy 74.5234375| Test Accuracy 74.63999938964844\n",
            "The iteration number is 31900 | and the loss it 0.8195298910140991| Trainning Accuracy 74.109375| Test Accuracy 74.62000274658203\n",
            "The iteration number is 32000 | and the loss it 0.8242068886756897| Trainning Accuracy 74.0859375| Test Accuracy 74.62000274658203\n",
            "The iteration number is 32100 | and the loss it 0.8263837099075317| Trainning Accuracy 74.0234375| Test Accuracy 74.58999633789062\n",
            "The iteration number is 32200 | and the loss it 0.7200867533683777| Trainning Accuracy 73.3125| Test Accuracy 74.62000274658203\n",
            "The iteration number is 32300 | and the loss it 0.7400676608085632| Trainning Accuracy 74.359375| Test Accuracy 74.63999938964844\n",
            "The iteration number is 32400 | and the loss it 0.8200978636741638| Trainning Accuracy 74.2265625| Test Accuracy 74.68000030517578\n",
            "The iteration number is 32500 | and the loss it 0.860005795955658| Trainning Accuracy 74.3046875| Test Accuracy 74.68000030517578\n",
            "The iteration number is 32600 | and the loss it 0.6583103537559509| Trainning Accuracy 73.7109375| Test Accuracy 74.69999694824219\n",
            "The iteration number is 32700 | and the loss it 0.688088595867157| Trainning Accuracy 74.34375| Test Accuracy 74.70999908447266\n",
            "The iteration number is 32800 | and the loss it 0.6174310445785522| Trainning Accuracy 74.578125| Test Accuracy 74.69000244140625\n",
            "The iteration number is 32900 | and the loss it 0.6066661477088928| Trainning Accuracy 74.46875| Test Accuracy 74.69999694824219\n",
            "The iteration number is 33000 | and the loss it 0.6786273121833801| Trainning Accuracy 74.3125| Test Accuracy 74.69999694824219\n",
            "The iteration number is 33100 | and the loss it 0.7663884162902832| Trainning Accuracy 73.9921875| Test Accuracy 74.77999877929688\n",
            "The iteration number is 33200 | and the loss it 0.6427913904190063| Trainning Accuracy 74.984375| Test Accuracy 75.31999969482422\n",
            "The iteration number is 33300 | and the loss it 0.7320346832275391| Trainning Accuracy 76.75| Test Accuracy 79.47999572753906\n",
            "The iteration number is 33400 | and the loss it 0.7239314913749695| Trainning Accuracy 80.28125| Test Accuracy 81.06999969482422\n",
            "The iteration number is 33500 | and the loss it 0.6544216275215149| Trainning Accuracy 80.9921875| Test Accuracy 81.37999725341797\n",
            "The iteration number is 33600 | and the loss it 0.5729424953460693| Trainning Accuracy 80.8671875| Test Accuracy 81.52999877929688\n",
            "The iteration number is 33700 | and the loss it 0.5518724918365479| Trainning Accuracy 81.5859375| Test Accuracy 81.66000366210938\n",
            "The iteration number is 33800 | and the loss it 0.663179337978363| Trainning Accuracy 81.6171875| Test Accuracy 81.6500015258789\n",
            "The iteration number is 33900 | and the loss it 0.8465471863746643| Trainning Accuracy 81.6875| Test Accuracy 81.66000366210938\n",
            "The iteration number is 34000 | and the loss it 0.49523013830184937| Trainning Accuracy 81.0625| Test Accuracy 81.80000305175781\n",
            "The iteration number is 34100 | and the loss it 0.8626931309700012| Trainning Accuracy 81.2734375| Test Accuracy 81.80999755859375\n",
            "The iteration number is 34200 | and the loss it 0.6565206050872803| Trainning Accuracy 81.59375| Test Accuracy 81.77999877929688\n",
            "The iteration number is 34300 | and the loss it 0.5219786763191223| Trainning Accuracy 81.2421875| Test Accuracy 81.87000274658203\n",
            "The iteration number is 34400 | and the loss it 0.552981436252594| Trainning Accuracy 81.6953125| Test Accuracy 81.87000274658203\n",
            "The iteration number is 34500 | and the loss it 0.640844464302063| Trainning Accuracy 81.46875| Test Accuracy 81.88999938964844\n",
            "The iteration number is 34600 | and the loss it 0.47125744819641113| Trainning Accuracy 81.8828125| Test Accuracy 81.88999938964844\n",
            "The iteration number is 34700 | and the loss it 0.8485303521156311| Trainning Accuracy 81.40625| Test Accuracy 81.97000122070312\n",
            "The iteration number is 34800 | and the loss it 0.6277304887771606| Trainning Accuracy 81.9921875| Test Accuracy 81.9800033569336\n",
            "The iteration number is 34900 | and the loss it 0.5971092581748962| Trainning Accuracy 81.3359375| Test Accuracy 82.01000213623047\n",
            "The iteration number is 35000 | and the loss it 0.8382145166397095| Trainning Accuracy 81.8515625| Test Accuracy 82.04000091552734\n",
            "The iteration number is 35100 | and the loss it 0.7067663669586182| Trainning Accuracy 81.7265625| Test Accuracy 82.1300048828125\n",
            "The iteration number is 35200 | and the loss it 0.47728997468948364| Trainning Accuracy 82.203125| Test Accuracy 82.05999755859375\n",
            "The iteration number is 35300 | and the loss it 0.7045919895172119| Trainning Accuracy 81.4921875| Test Accuracy 82.05999755859375\n",
            "The iteration number is 35400 | and the loss it 0.5867317914962769| Trainning Accuracy 81.890625| Test Accuracy 82.05000305175781\n",
            "The iteration number is 35500 | and the loss it 0.6008291244506836| Trainning Accuracy 81.75| Test Accuracy 82.1300048828125\n",
            "The iteration number is 35600 | and the loss it 0.559839129447937| Trainning Accuracy 81.59375| Test Accuracy 82.18000030517578\n",
            "The iteration number is 35700 | and the loss it 0.6061538457870483| Trainning Accuracy 81.796875| Test Accuracy 82.16000366210938\n",
            "The iteration number is 35800 | and the loss it 0.6585296392440796| Trainning Accuracy 82.515625| Test Accuracy 82.20000457763672\n",
            "The iteration number is 35900 | and the loss it 0.6596786379814148| Trainning Accuracy 81.96875| Test Accuracy 82.16999816894531\n",
            "The iteration number is 36000 | and the loss it 0.6802235841751099| Trainning Accuracy 81.7890625| Test Accuracy 82.20000457763672\n",
            "The iteration number is 36100 | and the loss it 0.6602248549461365| Trainning Accuracy 81.9453125| Test Accuracy 82.23999786376953\n",
            "The iteration number is 36200 | and the loss it 0.5182110071182251| Trainning Accuracy 81.796875| Test Accuracy 82.16000366210938\n",
            "The iteration number is 36300 | and the loss it 0.6774135828018188| Trainning Accuracy 82.0390625| Test Accuracy 82.22000122070312\n",
            "The iteration number is 36400 | and the loss it 0.6057080626487732| Trainning Accuracy 81.0625| Test Accuracy 82.23999786376953\n",
            "The iteration number is 36500 | and the loss it 0.5353043675422668| Trainning Accuracy 82.2734375| Test Accuracy 82.29000091552734\n",
            "The iteration number is 36600 | and the loss it 0.5068255066871643| Trainning Accuracy 82.2890625| Test Accuracy 82.31999969482422\n",
            "The iteration number is 36700 | and the loss it 0.5624388456344604| Trainning Accuracy 82.1171875| Test Accuracy 82.27000427246094\n",
            "The iteration number is 36800 | and the loss it 0.5863832831382751| Trainning Accuracy 81.515625| Test Accuracy 82.29000091552734\n",
            "The iteration number is 36900 | and the loss it 0.5622104406356812| Trainning Accuracy 82.3359375| Test Accuracy 82.27999877929688\n",
            "The iteration number is 37000 | and the loss it 0.4602420926094055| Trainning Accuracy 82.1484375| Test Accuracy 82.30000305175781\n",
            "The iteration number is 37100 | and the loss it 0.6123655438423157| Trainning Accuracy 81.828125| Test Accuracy 82.3800048828125\n",
            "The iteration number is 37200 | and the loss it 0.46781909465789795| Trainning Accuracy 81.8828125| Test Accuracy 82.3800048828125\n",
            "The iteration number is 37300 | and the loss it 0.5188435316085815| Trainning Accuracy 82.2421875| Test Accuracy 82.33000183105469\n",
            "The iteration number is 37400 | and the loss it 0.6044559478759766| Trainning Accuracy 82.1796875| Test Accuracy 82.3800048828125\n",
            "The iteration number is 37500 | and the loss it 0.5197060704231262| Trainning Accuracy 81.5234375| Test Accuracy 82.41000366210938\n",
            "The iteration number is 37600 | and the loss it 0.7851276993751526| Trainning Accuracy 81.921875| Test Accuracy 82.38999938964844\n",
            "The iteration number is 37700 | and the loss it 0.4985053241252899| Trainning Accuracy 82.203125| Test Accuracy 82.45000457763672\n",
            "The iteration number is 37800 | and the loss it 0.5212447047233582| Trainning Accuracy 82.59375| Test Accuracy 82.3800048828125\n",
            "The iteration number is 37900 | and the loss it 0.7016215324401855| Trainning Accuracy 81.515625| Test Accuracy 82.45999908447266\n",
            "The iteration number is 38000 | and the loss it 0.5372787714004517| Trainning Accuracy 81.8515625| Test Accuracy 82.4800033569336\n",
            "The iteration number is 38100 | and the loss it 0.5334742069244385| Trainning Accuracy 82.3125| Test Accuracy 82.4800033569336\n",
            "The iteration number is 38200 | and the loss it 0.6174126267433167| Trainning Accuracy 82.171875| Test Accuracy 82.48999786376953\n",
            "The iteration number is 38300 | and the loss it 0.5193281769752502| Trainning Accuracy 82.6953125| Test Accuracy 82.45999908447266\n",
            "The iteration number is 38400 | and the loss it 0.5796712636947632| Trainning Accuracy 82.1484375| Test Accuracy 82.47000122070312\n",
            "The iteration number is 38500 | and the loss it 0.5995697379112244| Trainning Accuracy 81.7578125| Test Accuracy 82.47000122070312\n",
            "The iteration number is 38600 | and the loss it 0.7733467817306519| Trainning Accuracy 81.6796875| Test Accuracy 82.51000213623047\n",
            "The iteration number is 38700 | and the loss it 0.5372275114059448| Trainning Accuracy 82.25| Test Accuracy 82.5\n",
            "The iteration number is 38800 | and the loss it 0.5972114205360413| Trainning Accuracy 82.5390625| Test Accuracy 82.51000213623047\n",
            "The iteration number is 38900 | and the loss it 0.40965884923934937| Trainning Accuracy 82.0234375| Test Accuracy 82.52999877929688\n",
            "The iteration number is 39000 | and the loss it 0.5895506143569946| Trainning Accuracy 82.484375| Test Accuracy 82.52000427246094\n",
            "The iteration number is 39100 | and the loss it 0.5460110902786255| Trainning Accuracy 82.3671875| Test Accuracy 82.54000091552734\n",
            "The iteration number is 39200 | and the loss it 0.5365232229232788| Trainning Accuracy 82.546875| Test Accuracy 82.56000518798828\n",
            "The iteration number is 39300 | and the loss it 0.6359198689460754| Trainning Accuracy 81.921875| Test Accuracy 82.55000305175781\n",
            "The iteration number is 39400 | and the loss it 0.583626389503479| Trainning Accuracy 82.3046875| Test Accuracy 82.56999969482422\n",
            "The iteration number is 39500 | and the loss it 0.7438673377037048| Trainning Accuracy 82.3125| Test Accuracy 82.58000183105469\n",
            "The iteration number is 39600 | and the loss it 0.5507331490516663| Trainning Accuracy 82.8828125| Test Accuracy 82.56999969482422\n",
            "The iteration number is 39700 | and the loss it 0.6992046236991882| Trainning Accuracy 81.7578125| Test Accuracy 82.56000518798828\n",
            "The iteration number is 39800 | and the loss it 0.7224797606468201| Trainning Accuracy 82.1875| Test Accuracy 82.55000305175781\n",
            "The iteration number is 39900 | and the loss it 0.6257498264312744| Trainning Accuracy 82.1796875| Test Accuracy 82.61000061035156\n",
            "The iteration number is 40000 | and the loss it 0.5831910371780396| Trainning Accuracy 81.640625| Test Accuracy 82.59000396728516\n",
            "The iteration number is 40100 | and the loss it 0.5928277373313904| Trainning Accuracy 82.5703125| Test Accuracy 82.62000274658203\n",
            "The iteration number is 40200 | and the loss it 0.8166365623474121| Trainning Accuracy 81.6640625| Test Accuracy 82.56999969482422\n",
            "The iteration number is 40300 | and the loss it 0.7454309463500977| Trainning Accuracy 81.5| Test Accuracy 82.54000091552734\n",
            "The iteration number is 40400 | and the loss it 0.37167006731033325| Trainning Accuracy 81.859375| Test Accuracy 82.55000305175781\n",
            "The iteration number is 40500 | and the loss it 0.8185807466506958| Trainning Accuracy 82.328125| Test Accuracy 82.56000518798828\n",
            "The iteration number is 40600 | and the loss it 0.5918432474136353| Trainning Accuracy 81.9921875| Test Accuracy 82.56000518798828\n",
            "The iteration number is 40700 | and the loss it 0.44741418957710266| Trainning Accuracy 82.171875| Test Accuracy 82.5999984741211\n",
            "The iteration number is 40800 | and the loss it 0.5290818214416504| Trainning Accuracy 81.6796875| Test Accuracy 82.59000396728516\n",
            "The iteration number is 40900 | and the loss it 0.5166500806808472| Trainning Accuracy 82.4296875| Test Accuracy 82.58000183105469\n",
            "The iteration number is 41000 | and the loss it 0.4639744758605957| Trainning Accuracy 82.59375| Test Accuracy 82.58000183105469\n",
            "The iteration number is 41100 | and the loss it 0.5022298693656921| Trainning Accuracy 82.0859375| Test Accuracy 82.62000274658203\n",
            "The iteration number is 41200 | and the loss it 0.5154263377189636| Trainning Accuracy 82.015625| Test Accuracy 82.59000396728516\n",
            "The iteration number is 41300 | and the loss it 0.7562071681022644| Trainning Accuracy 81.8515625| Test Accuracy 82.59000396728516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JYcVpX3R5Dl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fPIcIiDj5f0d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nsqb8mfC5Dn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7KeEcDfN5Drt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}